{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TToz9Cn3ADkd"
   },
   "source": [
    "# MLSS 2019: Neural Networks for Surface Meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yi_weSbaADke"
   },
   "source": [
    "This tutorial is based on ICCV 2019 paper \"Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape Representation Learning and Generation\" by Bouritsas, G., Bokhnyak, S., Bronstein, M., and Zafeiriou, S.\n",
    "\n",
    "It presents a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph.\n",
    "\n",
    "Using this operator, we will construct an autoencoder neural network for meshes and test its performanse on a set of human poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bz9jZYL-ADkf"
   },
   "source": [
    "<b>We are going to use Google Collab to run this notebook. In order to install all the necessary files run the following cells:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejG947vTghNr"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade git+https://github.com/mlss-skoltech/tutorials_week2.git#subdirectory=graph_neural_networks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40007,
     "status": "ok",
     "timestamp": 1567500301775,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "MFwkCzgXPwo5",
    "outputId": "c1177717-eaeb-44fb-b431-4f0cdd364c8c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip -q install trimesh==3.2 tensorboardX\n",
    "!wget -nc -O DFAUST.zip https://box.skoltech.ru/index.php/s/vcTrg71n94HkqjX/download\n",
    "!unzip -qn DFAUST.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_Z97wm2ADkn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "import pdb\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import trimesh\n",
    "from trimesh.exchange.export import export_mesh\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import noneuclideanlearning.mesh_sampling as mesh_sampling\n",
    "from noneuclideanlearning.utils import get_adj_trigs, generate_spirals, get_settings, show_mesh\n",
    "\n",
    "root_dir = './'\n",
    "mesh_template = trimesh.load('DFAUST/template/template.obj', process = False)\n",
    "dataset = 'DFAUST'\n",
    "name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXUk_MtBADkq"
   },
   "source": [
    "For this tutorial, we are going to need GPU. Let's make sure we have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1567500371427,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "SfgF4vflPwpF",
    "outputId": "c887ac16-ded0-4662-f21b-093885cc73fd"
   },
   "outputs": [],
   "source": [
    "GPU = True\n",
    "device_idx = 0\n",
    "torch.cuda.get_device_name(device_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZ700QK8ADkt"
   },
   "source": [
    "We use DFAUST for our experiments. The dynamic human body shape dataset consists of 40K+ 3D scans (6890 vertices) of ten unique identities performing actions such as leg and arm raises, jumps, etc. when we trained the network, we randomly split the data into a test set of 5000, 500 validation, and 34,5K+ train. Here, we only use 1000 training, 500 validation and 1000 testing samples.\n",
    "\n",
    "We are going to use a function for mesh visualization, and see what our data samples look like. We chose to do our analysis on human pose meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1565,
     "status": "ok",
     "timestamp": 1567500757128,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "rVD-BPcoPwpX",
    "outputId": "69a9c2f1-9224-4466-f58f-f576f56d69f6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_mesh(mesh_template.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPgnYqvOajS0"
   },
   "source": [
    "As you know, neural networks require a lot of parameters to work. We will set these parameters in ``utils.py`` file and import them here. Feel free to check the corresponding file to see the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IglcCoIePwpb"
   },
   "outputs": [],
   "source": [
    "args, reference_mesh_file, ds_factors, step_sizes,\\\n",
    "filter_sizes_enc, filter_sizes_dec, reference_points,\\\n",
    "summary_path, checkpoint_path, samples_path, prediction_path = get_settings(root_dir, dataset, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GGJaZjYADk2"
   },
   "source": [
    "In applications dealing with 3D data, the key challenge of geometric deep learning is a meaningful definition of intrinsic operations analogous to convolution and pooling on meshes or point clouds. Among numerous advantages of working directly on mesh or point cloud data is the fact that it is possible to build invariance to shape transformations (both rigid and nonrigid) into the architecture, as a result allowing to use significantly simpler models and much less training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnNiwvm2dWH7"
   },
   "source": [
    "`PyTorch`, as a number of other deep learning frameworks, needs to have a scpecial class for the `dataset` and `dataloader` objects. For the network we are going to implement, the dataset will take directory to the data, vertices of the meshes, and some other inputs, normalize the data inside, and output a training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWUj_PTaPwpf"
   },
   "outputs": [],
   "source": [
    "class autoencoder_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, points_dataset, shapedata, normalization = True, dummy_node = True):\n",
    "        \n",
    "        self.shapedata = shapedata\n",
    "        self.normalization = normalization\n",
    "        self.root_dir = root_dir\n",
    "        self.points_dataset = points_dataset\n",
    "        self.dummy_node = dummy_node\n",
    "        self.paths = np.load(os.path.join(root_dir, 'paths_'+points_dataset+'.npy'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        basename = self.paths[idx]\n",
    "        \n",
    "        verts_init = np.load(os.path.join(self.root_dir,'points'+'_'+self.points_dataset, basename+'.npy'))\n",
    "        if self.normalization:\n",
    "            verts_init = verts_init - self.shapedata.mean\n",
    "            verts_init = verts_init / self.shapedata.std\n",
    "        verts_init[np.where(np.isnan(verts_init))]=0.0\n",
    "        \n",
    "        verts_init = verts_init.astype('float32')\n",
    "        if self.dummy_node:\n",
    "            verts = np.zeros((verts_init.shape[0]+1,verts_init.shape[1]),dtype=np.float32)\n",
    "            verts[:-1,:] = verts_init\n",
    "            verts_init = verts\n",
    "        verts = torch.Tensor(verts_init)\n",
    "        \n",
    "\n",
    "        sample = {'points': verts}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hpBM9yxfsDA"
   },
   "source": [
    "## Neural networks for meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9bkU_vVADk6"
   },
   "source": [
    "One of the key challenges in developing convolution-like operators on graphs or manifolds is the lack of a global system of coordinates that can be associated with each point. In this tutorial, we will focus on fixed topology meshes. We will define an ordering-based graph convolutional operator, contrary to the permutation invariant operators in the literature of Graph Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1dm6f3cADk7"
   },
   "source": [
    "This way we obtain anisotropic filters without sacrificing computational complexity, while simultaneously we explicitly encode the fixed graph connectivity. The operator can potentially be generalised to other domains that accept implicit local orderings, such as arbitrary mesh topologies and point clouds, while it is naturally equivalent to traditional grid convolutions. Via this equivalence, common CNN practices, such as dilated convolutions, can be easily formulated for meshes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iooq4z1FgIhD"
   },
   "source": [
    "![spiral_conv](https://box.skoltech.ru/index.php/s/Fudde1kfbSvglJV/download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Of318_OLADk7"
   },
   "source": [
    "The issues of the absence of a global ordering and insensitivity to graph topology are irrelevant when dealing with fixed topology meshes. In particular, one can locally order the vertices and keep the order fixed. Then, graph convolution can be defined as follows:\n",
    "$$(f * g)_x = \\sum_{l=1}^L g_l f(x_l)$$\n",
    "where $\\{x_1, \\ldots, x_L\\}$ denote the neighbours of vertex $x$ ordered in a fixed way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdBxQLgqADk9"
   },
   "source": [
    "For example, it is possible to get the neighborhoods of mesh vertices through spiral ordering. Let $x \\in V$ be a mesh vertex, and let $R^d(x)$ be the $d$-ring, i.e. an ordered set of vertices whose shortest (graph) path to $x$ is exactly $d$ hops long; $R^d_j(x)$ denotes the $j$th element in the $d$-ring (trivially, $R^0_1(x) = x$). Then, one can define the spiral patch operator as the ordered sequence \n",
    "$$S(x) = \\{x, R^1_1(x), R^1_2(x), \\ldots , R^h_{|R^h|}\\},$$\n",
    "where $h$ denotes the patch radius, similar to the size of the kernel in classical CNNs. Then, spiral convolution is:\n",
    "$$(f * g)_x = \\sum_{l=1}^L g_l f(S_l(x)).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdRNr5g2Pwpj"
   },
   "outputs": [],
   "source": [
    "class SpiralConv(nn.Module):\n",
    "    def __init__(self, in_c, spiral_size, out_c, activation='elu', bias=True, device=None):\n",
    "        super(SpiralConv,self).__init__()\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "        self.device = device\n",
    "\n",
    "        self.conv = nn.Linear(in_c * spiral_size, out_c, bias=bias)\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(0.02)\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'identity':\n",
    "            self.activation = lambda x: x\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, spiral_adj):\n",
    "        bsize, num_pts, feats = x.size()\n",
    "        _, _, spiral_size = spiral_adj.size()\n",
    "  \n",
    "        spirals_index = spiral_adj.view(bsize * num_pts * spiral_size)  # [1d array of batch,vertx,vertx-adj]\n",
    "        batch_index = torch.arange(bsize, device=self.device) \\\n",
    "                           .view(-1, 1) \\\n",
    "                           .repeat([1,num_pts * spiral_size]) \\\n",
    "                           .view(-1).long()  # [0*numpt,1*numpt,etc.]\n",
    "        spirals = x[batch_index, spirals_index, :] \\\n",
    "                  .view(bsize * num_pts, spiral_size * feats) # [bsize*numpt, spiral*feats]\n",
    "\n",
    "        out_feat = self.conv(spirals)\n",
    "        out_feat = self.activation(out_feat)\n",
    "\n",
    "        out_feat = out_feat.view(bsize,num_pts,self.out_c)\n",
    "        zero_padding = torch.ones((1,x.size(1),1), device=self.device)\n",
    "        zero_padding[0,-1,0] = 0.0\n",
    "        out_feat = out_feat * zero_padding\n",
    "\n",
    "        return out_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIDRZ4zliKfD"
   },
   "source": [
    "The uniqueness of the ordering is given by fixing two degrees of freedom: the direction of the rings and the first vertex $R^1_1(x)$. The rest of the vertices of the spiral are ordered\n",
    "inductively. The direction is chosen by moving clockwise or counterclockwise, while the choice of the first vertex is based on the underlying geometry of the shape to ensure the robustness of the method. In particular, fixing a reference vertex $x_0$ on a template shape and choosing the initial point for each spiral to be in the direction of the shortest geodesic path to $x_0$, gives us\n",
    "$$R^1_1(x) = \\underset{y\\in R^1(x)}{\\operatorname{argmin}} d_{\\mathcal M}(x_0, y),$$\n",
    "where $d_{\\mathcal M}$ is the geodesic distance between two vertices on the mesh $\\mathcal M$. In order to allow for fixed-sized spirals, we choose a fixed length $L$ as a hyper-parameter and then either truncate or zero-pad each spiral depending on its size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GZ3I_PjADlE"
   },
   "source": [
    "Spiral convolution operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiiHBg6lhSpo"
   },
   "source": [
    "## Neural3DMM: mesh autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Otne1qzADlF"
   },
   "source": [
    "In essence, a Neural 3D Morphable Model is a deep convolutional mesh autoencoder, that learns hierarchical representations of a shape. This way one manages to learn semantically meaningful representations. Similar to traditional convolutional autoencoders, this one consists of a series of convolutional layers with small receptive fields followed by pooling and unpooling, for the encoder and the decoder respectively. A decimated or upsampled version of the mesh is obtained each time and the features of the existing vertices are either aggregated or extrapolated. The calculation of the features of the added vertices after upsampling is done through interpolation by weighting the nearby vertices with barycentric coordinates. The network is trained by minimising the $\\mathcal L_1$ norm between the input and the predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDPhW32RADk2"
   },
   "source": [
    "![image](https://box.skoltech.ru/index.php/s/k1xaTf5VlLYf2Na/download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxGD6pl3Pwpo"
   },
   "outputs": [],
   "source": [
    "class SpiralAutoencoder(nn.Module):\n",
    "    def __init__(self, filters_enc, filters_dec, latent_size, sizes, spiral_sizes, spirals, D, U, device, activation = 'elu'):\n",
    "        super(SpiralAutoencoder,self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.sizes = sizes\n",
    "        self.spirals = spirals\n",
    "        self.filters_enc = filters_enc\n",
    "        self.filters_dec = filters_dec\n",
    "        self.spiral_sizes = spiral_sizes\n",
    "        self.D = D\n",
    "        self.U = U\n",
    "        self.device = device\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.conv = []\n",
    "        input_size = filters_enc[0][0]\n",
    "        for i in range(len(spiral_sizes)-1):\n",
    "            if filters_enc[1][i]:\n",
    "                self.conv.append(SpiralConv(input_size, spiral_sizes[i], filters_enc[1][i],\n",
    "                                            activation=self.activation, device=device).to(device))\n",
    "                input_size = filters_enc[1][i]\n",
    "\n",
    "            self.conv.append(SpiralConv(input_size, spiral_sizes[i], filters_enc[0][i+1],\n",
    "                                        activation=self.activation, device=device).to(device))\n",
    "            input_size = filters_enc[0][i+1]\n",
    "\n",
    "        self.conv = nn.ModuleList(self.conv)   \n",
    "        \n",
    "        self.fc_latent_enc = nn.Linear((sizes[-1]+1)*input_size, latent_size)\n",
    "        self.fc_latent_dec = nn.Linear(latent_size, (sizes[-1]+1)*filters_dec[0][0])\n",
    "        \n",
    "        self.dconv = []\n",
    "        input_size = filters_dec[0][0]\n",
    "        for i in range(len(spiral_sizes)-1):\n",
    "            if i != len(spiral_sizes)-2:\n",
    "                self.dconv.append(SpiralConv(input_size, spiral_sizes[-2-i], filters_dec[0][i+1],\n",
    "                                             activation=self.activation, device=device).to(device))\n",
    "                input_size = filters_dec[0][i+1]  \n",
    "                \n",
    "                if filters_dec[1][i+1]:\n",
    "                    self.dconv.append(SpiralConv(input_size,spiral_sizes[-2-i], filters_dec[1][i+1],\n",
    "                                                 activation=self.activation, device=device).to(device))\n",
    "                    input_size = filters_dec[1][i+1]\n",
    "            else:\n",
    "                if filters_dec[1][i+1]:\n",
    "                    self.dconv.append(SpiralConv(input_size, spiral_sizes[-2-i], filters_dec[0][i+1],\n",
    "                                                 activation=self.activation, device=device).to(device))\n",
    "                    input_size = filters_dec[0][i+1]                      \n",
    "                    self.dconv.append(SpiralConv(input_size,spiral_sizes[-2-i], filters_dec[1][i+1],\n",
    "                                                 activation='identity', device=device).to(device)) \n",
    "                    input_size = filters_dec[1][i+1] \n",
    "                else:\n",
    "                    self.dconv.append(SpiralConv(input_size, spiral_sizes[-2-i], filters_dec[0][i+1],\n",
    "                                                 activation='identity', device=device).to(device))\n",
    "                    input_size = filters_dec[0][i+1]                      \n",
    "                    \n",
    "        self.dconv = nn.ModuleList(self.dconv)\n",
    "\n",
    "    def encode(self,x):\n",
    "        bsize = x.size(0)\n",
    "        S = self.spirals\n",
    "        D = self.D\n",
    "        \n",
    "        j = 0\n",
    "        for i in range(len(self.spiral_sizes)-1):\n",
    "            x = self.conv[j](x,S[i].repeat(bsize,1,1))\n",
    "            j+=1\n",
    "            if self.filters_enc[1][i]:\n",
    "                x = self.conv[j](x,S[i].repeat(bsize,1,1))\n",
    "                j+=1\n",
    "            x = torch.matmul(D[i],x)\n",
    "        x = x.view(bsize,-1)\n",
    "        return self.fc_latent_enc(x)\n",
    "    \n",
    "    def decode(self,z):\n",
    "        bsize = z.size(0)\n",
    "        S = self.spirals\n",
    "        U = self.U\n",
    "        \n",
    "        x = self.fc_latent_dec(z)\n",
    "        x = x.view(bsize,self.sizes[-1]+1,-1)\n",
    "        j=0\n",
    "        for i in range(len(self.spiral_sizes)-1):\n",
    "            x = torch.matmul(U[-1-i],x)\n",
    "            x = self.dconv[j](x,S[-2-i].repeat(bsize,1,1))\n",
    "            j+=1\n",
    "            if self.filters_dec[1][i+1]: \n",
    "                x = self.dconv[j](x,S[-2-i].repeat(bsize,1,1))\n",
    "                j+=1\n",
    "        return x\n",
    "\n",
    "    def forward(self,x):\n",
    "        bsize = x.size(0)\n",
    "        z = self.encode(x)\n",
    "        x = self.decode(z)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutEHeSVxXZs"
   },
   "source": [
    "Let's make a class for the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMlK7rhnPwpt"
   },
   "outputs": [],
   "source": [
    "class ShapeData(object):\n",
    "    def __init__(self, nVal, train_file, test_file, reference_mesh_file, normalization = True, load_flag = True, mean_subtraction_only = False):\n",
    "        self.nVal = nVal\n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.vertices_train = None\n",
    "        self.vertices_val = None\n",
    "        self.vertices_test = None\n",
    "        self.n_vertex = None\n",
    "        self.n_features = None\n",
    "        self.normalization = normalization\n",
    "        self.load_flag = load_flag\n",
    "        self.mean_subtraction_only = mean_subtraction_only\n",
    "        \n",
    "        if self.load_flag:\n",
    "            self.load()\n",
    "        self.reference_mesh = trimesh.load(reference_mesh_file, process = False)\n",
    "        \n",
    "        if self.load_flag:\n",
    "            self.mean = np.mean(self.vertices_train, axis=0)\n",
    "            self.std = np.std(self.vertices_train, axis=0)\n",
    "        else:\n",
    "            self.mean = None\n",
    "            self.std = None\n",
    "        self.normalize()\n",
    "        \n",
    "    def load(self):\n",
    "        vertices_train = np.load(self.train_file)\n",
    "        self.vertices_train = vertices_train[:-self.nVal]\n",
    "        self.vertices_val = vertices_train[-self.nVal:]\n",
    "\n",
    "        self.n_vertex = self.vertices_train.shape[1]\n",
    "        self.n_features = self.vertices_train.shape[2]\n",
    "\n",
    "        if os.path.exists(self.test_file):\n",
    "            self.vertices_test = np.load(self.test_file)\n",
    "            self.vertices_test = self.vertices_test\n",
    "\n",
    "    def normalize(self):\n",
    "        if self.load_flag:\n",
    "            if self.normalization:\n",
    "                if self.mean_subtraction_only:\n",
    "                    self.std = np.ones_like((self.std))\n",
    "                self.vertices_train = self.vertices_train - self.mean\n",
    "                self.vertices_train = self.vertices_train/self.std\n",
    "                self.vertices_train[np.where(np.isnan(self.vertices_train))]=0.0\n",
    "\n",
    "                self.vertices_val = self.vertices_val - self.mean\n",
    "                self.vertices_val = self.vertices_val/self.std\n",
    "                self.vertices_val[np.where(np.isnan(self.vertices_val))]=0.0\n",
    "\n",
    "                if self.vertices_test is not None:\n",
    "                    self.vertices_test = self.vertices_test - self.mean\n",
    "                    self.vertices_test = self.vertices_test/self.std\n",
    "                    self.vertices_test[np.where(np.isnan(self.vertices_test))]=0.0\n",
    "                \n",
    "                self.N = self.vertices_train.shape[0]\n",
    "\n",
    "                print('Vertices normalized')\n",
    "            else:\n",
    "                print('Vertices not normalized')\n",
    "\n",
    "\n",
    "    def save_meshes(self, filename, meshes, mesh_indices):\n",
    "        for i in range(meshes.shape[0]):\n",
    "            if self.normalization:\n",
    "                vertices = meshes[i].reshape((self.n_vertex, self.n_features))*self.std + self.mean\n",
    "            else:\n",
    "                vertices = meshes[i].reshape((self.n_vertex, self.n_features))\n",
    "            new_mesh = self.reference_mesh\n",
    "            if self.n_features == 3:\n",
    "                new_mesh.vertices = vertices\n",
    "            elif self.n_features == 6:\n",
    "                new_mesh.vertices = vertices[:,0:3]\n",
    "                colors = vertices[:,3:]\n",
    "                colors[np.where(colors<0)]=0\n",
    "                colors[np.where(colors>1)]=1\n",
    "                vertices[:,3:] = colors\n",
    "                new_mesh.visual = trimesh.visual.create_visual(vertex_colors = vertices[:,3:])\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            new_mesh.export(filename+'.'+str(mesh_indices[i]).zfill(6)+'.ply','ply')   \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVefLUlHxlW8"
   },
   "source": [
    "Now, we define the dataloaders for training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEiQd4lqPwpx"
   },
   "outputs": [],
   "source": [
    "def train_autoencoder_dataloader(dataloader_train, dataloader_val,\n",
    "                                 device, model, optim, loss_fn, \n",
    "                                 bsize, start_epoch, n_epochs, eval_freq, scheduler = None,\n",
    "                                 writer=None, save_recons=True, shapedata = None,\n",
    "                                 metadata_dir=None, samples_dir = None, checkpoint_path = None):\n",
    "    if not shapedata.normalization:\n",
    "        shapedata_mean = torch.Tensor(shapedata.mean).to(device)\n",
    "        shapedata_std = torch.Tensor(shapedata.std).to(device)\n",
    "    \n",
    "    total_steps = start_epoch*len(dataloader_train)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        tloss = []\n",
    "        for b, sample_dict in enumerate(tqdm(dataloader_train)):\n",
    "            optim.zero_grad()\n",
    "                \n",
    "            tx = sample_dict['points'].to(device)\n",
    "            cur_bsize = tx.shape[0]\n",
    "            \n",
    "            tx_hat = model(tx)\n",
    "            loss = loss_fn(tx, tx_hat)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            if shapedata.normalization:\n",
    "                tloss.append(cur_bsize * loss.item())\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    if shapedata.mean.shape[0]!=tx.shape[1]:\n",
    "                        tx_norm = tx[:,:-1,:]\n",
    "                        tx_hat_norm = tx_hat[:,:-1,:]\n",
    "                    else:\n",
    "                        tx_norm = tx\n",
    "                        tx_hat_norm = tx_hat\n",
    "                    tx_norm = (tx_norm - shapedata_mean)/shapedata_std\n",
    "                    tx_norm = torch.cat((tx_norm,torch.zeros(tx.shape[0],1,tx.shape[2]).to(device)),1)\n",
    "                    \n",
    "                    tx_hat_norm = (tx_hat_norm -shapedata_mean)/shapedata_std\n",
    "                    tx_hat_norm = torch.cat((tx_hat_norm,torch.zeros(tx.shape[0],1,tx.shape[2]).to(device)),1)\n",
    "                    \n",
    "                    loss_norm = loss_fn(tx_norm, tx_hat_norm)\n",
    "                    tloss.append(cur_bsize * loss_norm.item())\n",
    "            if writer and total_steps % eval_freq == 0:\n",
    "                writer.add_scalar('loss/loss/data_loss',loss.item(),total_steps)\n",
    "                writer.add_scalar('training/learning_rate', optim.param_groups[0]['lr'],total_steps)\n",
    "            total_steps += 1\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        vloss = []\n",
    "        with torch.no_grad():\n",
    "            for b, sample_dict in enumerate(tqdm(dataloader_val)):\n",
    "\n",
    "                tx = sample_dict['points'].to(device)\n",
    "                cur_bsize = tx.shape[0]\n",
    "\n",
    "                tx_hat = model(tx)               \n",
    "                loss = loss_fn(tx, tx_hat)\n",
    "                \n",
    "                if shapedata.normalization:\n",
    "                    vloss.append(cur_bsize * loss.item())\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        if shapedata.mean.shape[0]!=tx.shape[1]:\n",
    "                            tx_norm = tx[:,:-1,:]\n",
    "                            tx_hat_norm = tx_hat[:,:-1,:]\n",
    "                        else:\n",
    "                            tx_norm = tx\n",
    "                            tx_hat_norm = tx_hat\n",
    "                        tx_norm = (tx_norm - shapedata_mean)/shapedata_std\n",
    "                        tx_norm = torch.cat((tx_norm,torch.zeros(tx.shape[0],1,tx.shape[2]).to(device)),1)\n",
    "                    \n",
    "                        tx_hat_norm = (tx_hat_norm - shapedata_mean)/shapedata_std\n",
    "                        tx_hat_norm = torch.cat((tx_hat_norm,torch.zeros(tx.shape[0],1,tx.shape[2]).to(device)),1)\n",
    "                    \n",
    "                        loss_norm = loss_fn(tx_norm, tx_hat_norm)\n",
    "                        vloss.append(cur_bsize * loss_norm.item())   \n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        epoch_tloss = sum(tloss) / float(len(dataloader_train.dataset))\n",
    "        writer.add_scalar('avg_epoch_train_loss',epoch_tloss,epoch)\n",
    "        if len(dataloader_val.dataset) > 0:\n",
    "            epoch_vloss = sum(vloss) / float(len(dataloader_val.dataset))\n",
    "            writer.add_scalar('avg_epoch_valid_loss', epoch_vloss,epoch)\n",
    "            print('epoch {0} | tr {1} | val {2}'.format(epoch,epoch_tloss,epoch_vloss))\n",
    "        else:\n",
    "            print('epoch {0} | tr {1} '.format(epoch,epoch_tloss))\n",
    "        model = model.cpu()\n",
    "  \n",
    "        torch.save({'epoch': epoch,\n",
    "            'autoencoder_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict' : optim.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "        },os.path.join(metadata_dir, checkpoint_path+'.pth.tar'))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({'epoch': epoch,\n",
    "            'autoencoder_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict' : optim.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            },os.path.join(metadata_dir, checkpoint_path+'%s.pth.tar'%(epoch)))\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        if save_recons:\n",
    "            with torch.no_grad():\n",
    "                if epoch == 0:\n",
    "                    mesh_ind = [0]\n",
    "                    msh = tx[mesh_ind[0]:1,0:-1,:].detach().cpu().numpy()\n",
    "                    shapedata.save_meshes(os.path.join(samples_dir,'input_epoch_{0}'.format(epoch)),\n",
    "                                                     msh, mesh_ind)\n",
    "                mesh_ind = [0]\n",
    "                msh = tx_hat[mesh_ind[0]:1,0:-1,:].detach().cpu().numpy()\n",
    "                shapedata.save_meshes(os.path.join(samples_dir,'epoch_{0}'.format(epoch)),\n",
    "                                                 msh, mesh_ind)\n",
    "\n",
    "    print('~FIN~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0sK3nIbPwp0"
   },
   "outputs": [],
   "source": [
    "def test_autoencoder_dataloader(device, model, dataloader_test, shapedata, mm_constant = 1000):\n",
    "    model.eval()\n",
    "    l1_loss = 0\n",
    "    l2_loss = 0\n",
    "    shapedata_mean = torch.Tensor(shapedata.mean).to(device)\n",
    "    shapedata_std = torch.Tensor(shapedata.std).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, sample_dict in enumerate(tqdm(dataloader_test)):\n",
    "            tx = sample_dict['points'].to(device)\n",
    "            prediction = model(tx)  \n",
    "            if i==0:\n",
    "                predictions = copy.deepcopy(prediction)\n",
    "            else:\n",
    "                predictions = torch.cat([predictions,prediction],0) \n",
    "                \n",
    "            if dataloader_test.dataset.dummy_node:\n",
    "                x_recon = prediction[:,:-1]\n",
    "                x = tx[:,:-1]\n",
    "            else:\n",
    "                x_recon = prediction\n",
    "                x = tx\n",
    "            l1_loss+= torch.mean(torch.abs(x_recon-x))*x.shape[0]/float(len(dataloader_test.dataset))\n",
    "            \n",
    "            x_recon = (x_recon * shapedata_std + shapedata_mean) * mm_constant\n",
    "            x = (x * shapedata_std + shapedata_mean) * mm_constant\n",
    "            l2_loss+= torch.mean(torch.sqrt(torch.sum((x_recon - x)**2,dim=2)))*x.shape[0]/float(len(dataloader_test.dataset))\n",
    "            \n",
    "        predictions = predictions.cpu().numpy()\n",
    "        l1_loss = l1_loss.item()\n",
    "        l2_loss = l2_loss.item()\n",
    "    \n",
    "    return predictions, l1_loss, l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yr4OemJBj3lW"
   },
   "source": [
    "## Training the network\n",
    "### Preliminary computations and loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x118wwbQxv_W"
   },
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1567501718487,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "r_BmREEYPwp3",
    "outputId": "031a7495-fa1b-49f8-df0d-38fc7290054b"
   },
   "outputs": [],
   "source": [
    "np.random.seed(args['seed'])\n",
    "print(\"Loading data .. \")\n",
    "\n",
    "shapedata = ShapeData(nVal=args['nVal'], \n",
    "                     train_file=args['data']+'/train.npy',\n",
    "                     test_file=args['data']+'/test.npy', \n",
    "                     reference_mesh_file=args['reference_mesh_file'],\n",
    "                     normalization = args['normalization'], load_flag = False)\n",
    "shapedata.mean = np.load(args['data']+'/mean.npy')\n",
    "shapedata.std = np.load(args['data']+'/std.npy')\n",
    "shapedata.n_vertex = shapedata.mean.shape[0]\n",
    "shapedata.n_features = shapedata.mean.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_gihuxYxxx8"
   },
   "source": [
    "Load the precalculated transform matrices for the mesh models we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1567501733523,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "l5h4rPq5Pwp6",
    "outputId": "a20e8be1-e236-46e3-b2c8-4016d6b84883"
   },
   "outputs": [],
   "source": [
    "print(\"Loading Transform Matrices ..\")\n",
    "with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'rb') as fp:\n",
    "    downsampling_matrices = pickle.load(fp,encoding = 'latin1')\n",
    "#         downsampling_matrices = pickle.load(fp)\n",
    "\n",
    "M_verts_faces = downsampling_matrices['M_verts_faces']\n",
    "M = [trimesh.base.Trimesh(vertices=M_verts_faces[i][0], faces=M_verts_faces[i][1], process = False) for i in range(len(M_verts_faces))]\n",
    "A = downsampling_matrices['A']\n",
    "D = downsampling_matrices['D']\n",
    "U = downsampling_matrices['U']\n",
    "F = downsampling_matrices['F']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HR1UN4uox5aV"
   },
   "source": [
    "Precompute the reference points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1567501742984,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "tB7_SG0LPwp9",
    "outputId": "4fec0d29-3800-456b-c5b1-ac4b7a089cb6"
   },
   "outputs": [],
   "source": [
    "print(\"Calculating reference points for downsampled versions..\")\n",
    "for i in range(len(args['ds_factors'])):\n",
    "    dist = euclidean_distances(M[i+1].vertices, M[0].vertices[reference_points[0]])\n",
    "    reference_points.append(np.argmin(dist,axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xF3UjCu3x_F-"
   },
   "source": [
    "Generating spiral paths for the convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1567501757931,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "HavkDpymPwqB",
    "outputId": "acd2f4fe-488d-4d48-a0f0-cddf067d65da",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sizes = [x.vertices.shape[0] for x in M]\n",
    "Adj, Trigs = get_adj_trigs(A, F, shapedata.reference_mesh)\n",
    "\n",
    "spirals_np, spiral_sizes,spirals = generate_spirals(args['step_sizes'], \n",
    "                                                    M, Adj, Trigs, \n",
    "                                                    reference_points = reference_points, \n",
    "                                                    dilation = args['dilation'], random = False, \n",
    "                                                    counter_clockwise = True)\n",
    "\n",
    "bU = []\n",
    "bD = []\n",
    "for i in range(len(D)):\n",
    "    d = np.zeros((1,D[i].shape[0]+1,D[i].shape[1]+1))\n",
    "    u = np.zeros((1,U[i].shape[0]+1,U[i].shape[1]+1))\n",
    "    d[0,:-1,:-1] = D[i].todense()\n",
    "    u[0,:-1,:-1] = U[i].todense()\n",
    "    d[0,-1,-1] = 1\n",
    "    u[0,-1,-1] = 1\n",
    "    bD.append(d)\n",
    "    bU.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7805,
     "status": "ok",
     "timestamp": 1567501790055,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "eZn3ILUAPwqE",
    "outputId": "017feb5e-d7e3-4aeb-fb0e-fa25a8f19dfb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "tspirals = [torch.from_numpy(s).long().to(device) for s in spirals_np]\n",
    "tD = [torch.from_numpy(s).float().to(device) for s in bD]\n",
    "tU = [torch.from_numpy(s).float().to(device) for s in bU]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0GVNVuNADll"
   },
   "source": [
    "The last steps before we can use the network: initialize the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7_87SncPwqI"
   },
   "outputs": [],
   "source": [
    "# Building model, optimizer, and loss function\n",
    "\n",
    "dataset_train = autoencoder_dataset(root_dir = args['data'], points_dataset = 'train',\n",
    "                                           shapedata = shapedata,\n",
    "                                           normalization = args['normalization'])\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=args['batch_size'],\\\n",
    "                                     shuffle = args['shuffle'], num_workers = args['num_workers'])\n",
    "\n",
    "dataset_val = autoencoder_dataset(root_dir = args['data'], points_dataset = 'val', \n",
    "                                         shapedata = shapedata,\n",
    "                                         normalization = args['normalization'])\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=args['batch_size'],\\\n",
    "                                     shuffle = False, num_workers = args['num_workers'])\n",
    "\n",
    "\n",
    "dataset_test = autoencoder_dataset(root_dir = args['data'], points_dataset = 'test',\n",
    "                                          shapedata = shapedata,\n",
    "                                          normalization = args['normalization'])\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=args['batch_size'],\\\n",
    "                                     shuffle = False, num_workers = args['num_workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYO7U8nUyuyF"
   },
   "source": [
    "Here is the neural network itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zze6oycTPwqL"
   },
   "outputs": [],
   "source": [
    "model = SpiralAutoencoder(filters_enc = args['filter_sizes_enc'],   \n",
    "                          filters_dec = args['filter_sizes_dec'],\n",
    "                          latent_size=args['nz'],\n",
    "                          sizes=sizes,\n",
    "                          spiral_sizes=spiral_sizes,\n",
    "                          spirals=tspirals,\n",
    "                          D=tD, U=tU,device=device).to(device)\n",
    " \n",
    "    \n",
    "optim = torch.optim.Adam(model.parameters(),lr=args['lr'],weight_decay=args['regularization'])\n",
    "scheduler=torch.optim.lr_scheduler.StepLR(optim, args['decay_steps'],gamma=args['decay_rate'])\n",
    "\n",
    "def loss_l1(outputs, targets):\n",
    "    L = torch.abs(outputs - targets).mean()\n",
    "    return L \n",
    "loss_fn = loss_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1567501831494,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "kBQ3nRq5PwqO",
    "outputId": "31589245-1878-4c94-aa21-791105f1e58c"
   },
   "outputs": [],
   "source": [
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params)) \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPR2Fn4dy0f-"
   },
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka_-fBe75Y67"
   },
   "source": [
    "Here we provide the code for training the network, but since it takes a lot of data and time to train it properly, we will use the pretraied model to see the main results. For this, we set the parameter `mode` to `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atY4eI3lPwqR"
   },
   "outputs": [],
   "source": [
    "args['mode'] = 'test'\n",
    "args['shuffle'] = True\n",
    "args['resume'] = True\n",
    "if args['mode'] == 'train':\n",
    "    writer = SummaryWriter(summary_path)\n",
    "    with open(os.path.join(args['results_folder'],'checkpoints', args['name'] +'_params.json'),'w') as fp:\n",
    "        saveparams = copy.deepcopy(args)\n",
    "        json.dump(saveparams, fp)\n",
    "        \n",
    "    if args['resume']:\n",
    "            print('loading checkpoint from file %s'%(os.path.join(checkpoint_path,args['checkpoint_file'])))\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar'),map_location=device)\n",
    "            start_epoch = checkpoint_dict['epoch'] + 1\n",
    "            model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
    "            optim.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(checkpoint_dict['scheduler_state_dict'])\n",
    "            print('Resuming from epoch %s'%(str(start_epoch)))     \n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        \n",
    "\n",
    "    train_autoencoder_dataloader(dataloader_train, dataloader_val,\n",
    "                          device, model, optim, loss_fn,\n",
    "                          bsize = args['batch_size'],\n",
    "                          start_epoch = start_epoch,\n",
    "                          n_epochs = args['num_epochs'],\n",
    "                          eval_freq = args['eval_frequency'],\n",
    "                          scheduler = scheduler,\n",
    "                          writer = writer,\n",
    "                          save_recons=True,\n",
    "                          shapedata=shapedata,\n",
    "                          metadata_dir=checkpoint_path, samples_dir=samples_path,\n",
    "                          checkpoint_path = args['checkpoint_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuLdpoeXkQ5j"
   },
   "source": [
    "### Testing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3895,
     "status": "ok",
     "timestamp": 1567501925397,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "--1U0awpPwqT",
    "outputId": "2b0f25fb-55eb-42c0-9ae5-304f5d60f32d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args['mode'] == 'test':\n",
    "    print('loading checkpoint from file %s'%(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar')))\n",
    "    checkpoint_dict = torch.load(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar'),map_location=device)\n",
    "    model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
    "        \n",
    "    predictions, norm_l1_loss, l2_loss = test_autoencoder_dataloader(device, model, dataloader_test, \n",
    "                                                                     shapedata, mm_constant = 1000)    \n",
    "    np.save(os.path.join(prediction_path,'predictions'), predictions)   \n",
    "        \n",
    "    print('autoencoder: normalized loss', norm_l1_loss)\n",
    "    \n",
    "    print('autoencoder: euclidean distance in mm=', l2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSJg587RkW09"
   },
   "source": [
    "## Linear algebra in the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAJA8f5HlF1m"
   },
   "source": [
    "### Sampling from the latent space\n",
    "_The purpose of this part is to learn how to sample from a latent space of mesh encodings that we have created._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rox6OYi_E61U"
   },
   "source": [
    "The autoencoder architecture is designed such that it is able to reconstruct shapes from any point in the latent space. This means, that we can sample shapes from the latent space. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exersize:** generate a random 16-dimensional vector ysing pytorch and use this vector to decode the underlying shape. \n",
    "\n",
    "**Hint:** use `numpy.random.RandomState` and a lucky seed is `34563`. \n",
    "\n",
    "**Hint:** use `model.decode` to restore the mesh structure using the learned filters.\n",
    "\n",
    "**Hint:** transform the decoded mesh into standardized form using `mesh_mean` and `mesh_std` learned by `shapedata` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_latent_space(model, mesh_mean, mesh_std, dim=16, rand=None):\n",
    "    \"\"\"\n",
    "    Samples from the latent space of the model. \n",
    "    \n",
    "    model: the shape autoencoder model having \".decode\" method \n",
    "           (note that this method only accepts 2D tensors)\n",
    "    \n",
    "    mesh_mean: 6890-dim vector of mean vertex parameters\n",
    "\n",
    "    mesh_std: 6890-dim vector of vertex scales\n",
    "    \n",
    "    dim: dimensionality of the latent space\n",
    "    \n",
    "    rand: an object holding the `.uniform` methods\n",
    "    \"\"\"\n",
    "    if None is rand:\n",
    "        r = np.random\n",
    "    else:\n",
    "        r = rand\n",
    "    \n",
    "    # Step 1: generate a uniformly random vector of shape `dim`. \n",
    "    # Note: turn this vector into a matrix of size [1, dim]\n",
    "    # Note: `r` variable has the `uniform` method\n",
    "    \n",
    "    ####################################################\n",
    "    ### YOUR CODE HERE ### YOUR CODE HERE ###\n",
    "    ####################################################\n",
    "    \n",
    "    \n",
    "    # Step 2: decode the latent code representation into a \n",
    "    # Note: the model outputs a batch of size [batch_size, n_vertices, 3]\n",
    "    # Note: remove the added zeroth vertex\n",
    "\n",
    "    ####################################################\n",
    "    ### YOUR CODE HERE ### YOUR CODE HERE ###\n",
    "    ####################################################\n",
    "    \n",
    "    \n",
    "    # Step 3: scale the mesh_std and add mesh_mean \n",
    "    \n",
    "    ####################################################\n",
    "    ### YOUR CODE HERE ### YOUR CODE HERE ###\n",
    "    ####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1608,
     "status": "ok",
     "timestamp": 1567501935381,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "XBJ5dHTSPwqv",
    "outputId": "9b9de1b9-6ee9-4f57-8287-d033c5cb45e0"
   },
   "outputs": [],
   "source": [
    "restored = sample_from_latent_space(model, shapedata.mean, shapedata.std)\n",
    "\n",
    "show_mesh(restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring individual latent transformations\n",
    "_The purpose of this part is to explore separate directions in the latent space._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_code_np = np.random.RandomState(34563).uniform(-5, 5, size=16)\n",
    "latent_code_np = np.expand_dims(latent_code_np, axis=0)\n",
    "latent_code = torch.Tensor(latent_code_np).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exersize:** explore the specific transformations of the latent code to interpret the meaning of its parts. Identify:\n",
    " - which transformations correspond to subjects with raised/lowered hands? \n",
    " - .. to subjects leaning back? \n",
    " - .. to subject gender?\n",
    "\n",
    "**Hint:** vary separate coordinates in the `latent_code` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1696,
     "status": "ok",
     "timestamp": 1567502009278,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "rXoUMWVJPwqx",
    "outputId": "e8829ab6-ba37-4227-a8e6-1d5a75d591f1"
   },
   "outputs": [],
   "source": [
    "### backup the previous latent code \n",
    "latent_copy = copy.deepcopy(latent_code)\n",
    "\n",
    "# manually edit the code \n",
    "####################################################\n",
    "### YOUR CODE HERE ### YOUR CODE HERE ###\n",
    "####################################################\n",
    "\n",
    "def decode_mesh(latent_code, model, mesh_mean, mesh_std):\n",
    "    restored_np = model.decode(latent_code).detach().cpu().numpy()\n",
    "    restored_np = np.squeeze(restored_np)[:-1]\n",
    "    restored_np = restored_np * mesh_std + mesh_mean\n",
    "    return restored_np\n",
    "\n",
    "# decode the mesh using the helper\n",
    "restored_copy = decode_mesh(latent_copy, model, shapedata.mean, shapedata.std)\n",
    "\n",
    "show_mesh(restored_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNNRcXAzlLj8"
   },
   "source": [
    "### Temporal relations between poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqUbCQaCkjt0"
   },
   "source": [
    "Let us load the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzZAFEgzPwqX"
   },
   "outputs": [],
   "source": [
    "test = np.load('DFAUST/preprocessed/test.npy')\n",
    "\n",
    "mesh_female_pose = test[31]\n",
    "mesh_female_neutral = test[781]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpJ53fvbkoyq"
   },
   "source": [
    "Let's see the properties of the latent space. For example, here is the reference female pose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1202,
     "status": "ok",
     "timestamp": 1567502019896,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "DAkGa9y1PwqZ",
    "outputId": "50b1856e-6f23-4fdb-c83d-5446c8664553"
   },
   "outputs": [],
   "source": [
    "show_mesh(mesh_female_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UinoGcZBKDbs"
   },
   "source": [
    "Here is the neutral female pose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1556,
     "status": "ok",
     "timestamp": 1567502085634,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "Y4Z-w5cJPwqc",
    "outputId": "fbc99f6e-1660-431a-a4ed-025307a70073"
   },
   "outputs": [],
   "source": [
    "show_mesh(mesh_female_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XPn3BsiOTtx"
   },
   "source": [
    "Let's encode there two shapes, and in the latent space, have some algebraic operations on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_mesh(mesh, model, mesh_mean, mesh_std):\n",
    "    mesh = (mesh - mesh_mean) / mesh_std  # standardize mesh\n",
    "    \n",
    "    mesh = np.vstack([mesh, np.array([0, 0, 0])])  # add the necessary zeroth vertex \n",
    "    mesh = np.expand_dims(mesh, axis=0)  # batch of size 1\n",
    "    \n",
    "    mesh_tensor = torch.Tensor(mesh).to(model.device)  # convert to torch tensor\n",
    "\n",
    "    latent_code = model.encode(mesh_tensor)  # do a forward pass\n",
    "    \n",
    "    return latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_female_pose = encode_mesh(mesh_female_pose, model, shapedata.mean, shapedata.std)\n",
    "latent_female_neutral = encode_mesh(mesh_female_neutral, model, shapedata.mean, shapedata.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkEtWFKpKUia"
   },
   "source": [
    "The latent space allows us to slightly alter shapes. For example, we can interpolate between two poses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exersize:** program a pose interpolation function to build intermediate shapes.\n",
    "\n",
    "**Hint:** use a simple convex combination of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_between(latent_1, latent_2, w1=0., w2=1.):\n",
    "    \"\"\"\n",
    "    Weighs latent_1 and latent_2 by w1 and w2, respectively.    \n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    ### YOUR CODE HERE ### YOUR CODE HERE ###\n",
    "    ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_code = interpolate_between(latent_female_pose, latent_female_neutral, w1=0.9, w2=0.1)\n",
    "\n",
    "interpolated_mesh = decode_mesh(interpolated_code, model, shapedata.mean, shapedata.std)\n",
    "\n",
    "show_mesh(interpolated_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgLdL9zOKnF2"
   },
   "source": [
    "And, having some neutral pose and reference pose, we can extrapolate the movement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exersize:** program a pose **extra**polation function to build intermediate shapes.\n",
    "\n",
    "**Hint:** use a simple convex combination of the two but forget about positiveness of the weigths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1964,
     "status": "ok",
     "timestamp": 1567502175423,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "BpGNgWVUPwqr",
    "outputId": "127b996f-2063-4879-da7f-7cee9748c968"
   },
   "outputs": [],
   "source": [
    "extrapolated_code = interpolate_between(\n",
    "    ####################################################\n",
    "    ### YOUR CODE HERE ### YOUR CODE HERE ###\n",
    "    ####################################################\n",
    ")\n",
    "\n",
    "extrapolated_mesh = decode_mesh(extrapolated_code, model, shapedata.mean, shapedata.std)\n",
    "\n",
    "show_mesh(extrapolated_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h90sCG0LMwR"
   },
   "source": [
    "\n",
    "### Pose transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYhynozHKGbu"
   },
   "source": [
    "Lastly, let's see the pose transfer example. Taking the reference and the neutral female poses let's transfer the reference pose to a different body shape using this neutral male pose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_male_neutral = test[888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1732,
     "status": "ok",
     "timestamp": 1567502216083,
     "user": {
      "displayName": "Alexey Artemov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAXpT9QtCkx-BfldAWvC1RVI5z9dgWA5LEDhoIuLg=s64",
      "userId": "14786261649952347117"
     },
     "user_tz": -180
    },
    "id": "ew4KkiT5Pwqf",
    "outputId": "0fdbadc2-11b6-4cdb-93ac-3555685ae0e6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_mesh(mesh_male_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zr1MAANSKJCj"
   },
   "source": [
    "Now, using a simple algebaric relations, we substract neutral female pose from the initial pose and add the neutral male pose. Let's see the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** perform pose transfer between female and male meshes:\n",
    " - encode female neutral and pose meshes into latent space\n",
    " - encode neutral male mesh into latent space\n",
    " - perform latent space arithmetic to compute an approximation \n",
    " \n",
    "\n",
    " **Hint:** first compute the difference between latent codes representing pose and neutral female meshes, then add this difference to the neutral male latent code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_female_pose = encode_mesh(test[31], model, shapedata.mean, shapedata.std)\n",
    "latent_female_neutral = encode_mesh(test[781], model, shapedata.mean, shapedata.std)\n",
    "latent_male_neutral = encode_mesh(test[888], model, shapedata.mean, shapedata.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_pose = # your code here\n",
    "latent_male_pose = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_male_pose = decode_mesh(latent_male_pose, model, shapedata.mean, shapedata.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mesh(mesh_male_pose)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of neural3DMM.ipynb",
   "provenance": [
    {
     "file_id": "18EelnRaf9p3OCcxHAGad365GpsmDS4r1",
     "timestamp": 1567500231199
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
