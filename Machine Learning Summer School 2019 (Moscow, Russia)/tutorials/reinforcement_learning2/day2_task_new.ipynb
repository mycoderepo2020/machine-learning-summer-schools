{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NinaMaz/mlss-tutorials/blob/master/whiteson/day2_solutions_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1kukcFJl_iV"
   },
   "source": [
    "<h1> DAY 2: Approximate Reinforcement Learning </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "6sMLFAalfIOV",
    "outputId": "75003c0f-cb66-4f0b-9618-e8ee817f615c"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade git+https://github.com/mlss-skoltech/tutorials_week2.git#subdirectory=reinforcement_learning2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTrudAJSfGPq"
   },
   "source": [
    "<p> <img src=\"http://skyai.org/wiki/?plugin=attach&refer=Documentation%2FTutorial%20-%20Example%20-%20Mountain%20Car&openfile=mountaincar.png\" alt=\"picture of the mountain-car environment\" style=\"float:right;width:500px;border:10px solid #FFFFFF;\">\n",
    "<h3> MountainCar </h3>\n",
    "<p> A car starts in a valley between two mountains, as depicted in the image to the right. The car must reach the goal location on the top of the right mountain by using three possible actions: accelerate forwards, backwards or doing nothing. However, the car's motor is underactuated and cannot drive straight uphill. The agent's task is to find a policy that swings the car back and forth such that it eventually reaches the goal position. </p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZbylA87j5UQ"
   },
   "source": [
    "Let us denote the <i>set of states, set of actions and a reward function</i> for MountainCar environment:\n",
    "\n",
    "***S*** : position of the car (min coordinate is <b>-1.2</b>; max coordinate is <b>0.6</b>] and its velocity (min velocity is <b>-0.07</b>, max velocity is <b>0.07</b>) (notice that the set of states is continious)\n",
    "\n",
    "***A*** : force (also continious) with which you push left (<b>-1</b>), do not push(<b>0</b>), push right (<b>1</b>)\n",
    "\n",
    "***R*** : reward for hiting the goal is <b>100</b>, for taking a step it is $-0.1 \\times a^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSHWiQvakHVF"
   },
   "source": [
    "<b>Important Note:</b> <p> Episodes end after reaching the goal or 1000 steps otherwise (this is a deviation from the classical task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtOsos9ufGPt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "# import random\n",
    "\n",
    "from math import e as nate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69geUJz_fGPv"
   },
   "outputs": [],
   "source": [
    "def make_environment():\n",
    "    \"\"\" Returns a new mountain-car environment. \"\"\"\n",
    "    # Register special version of MountainCar with long episodes\n",
    "    gym.envs.register(\n",
    "        id='MountainCarExtraLong-v0',\n",
    "        entry_point='gym.envs.classic_control:MountainCarEnv',\n",
    "        max_episode_steps=1000,\n",
    "        reward_threshold=-110.0,\n",
    "    )\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make('MountainCarExtraLong-v0')\n",
    "    env_name = 'MountainCarExtraLong-v0'\n",
    "    env = gym.make(env_name)\n",
    "    # Set seeds (for reproduceability)\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    return getattr(env, \"env\", env)\n",
    "\n",
    "# Make a mountain-car environment\n",
    "env = make_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UD7iIjGNfGPs"
   },
   "source": [
    "# EXERCISE 1: Tabular Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4REOVpGGkToi"
   },
   "source": [
    "***TD-error formula***\n",
    "$$Q(s,a) \\longleftarrow Q(s,a) + \\alpha[{\\color{red}{r + \\gamma \\max_{a'}Q(s',a')}}\\,{\\color{blue}{-\\,Q(s,a)}}]$$\n",
    "\n",
    "***Tabular Q-learning-algorithm***\n",
    "\n",
    "<img src = 'img/tabular.png'>\n",
    "\n",
    "> [Shimon Whiteson lecture 1, p.62](https://github.com/mlss-skoltech/lectures/blob/master/reinforcement_learning/lecture1_tabular_methods.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVuWKFgDkY_M"
   },
   "source": [
    "In this exercise, you will familiarise yourself with a given agent class and an experiment script that runs OpenAI Gym environments.\n",
    "\n",
    "In the following code block, we define a longer Mountain-Car environment from the OpenAI Gym.\n",
    "Note that some installed libraries may throw some warnings here. Ignore them unless task 1c does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpT4ijJ6fGPx"
   },
   "source": [
    "## TASK 1a: the agent \n",
    "Familiarise yourself with the below specification of a QLearner at the example of the following Tabular Q-Learner. \n",
    "Given the action to execute, gym environments return the next state, the received reward and a Boolean done, which indicated that the episode has ended.\n",
    "\n",
    "Complete the update() function of the TabularQLearner by filling in the TD-error for Q-learning. Make sure that the value of the next state is 0 for the last step of an episode, that is, when done=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QcTaTUYDfae7"
   },
   "outputs": [],
   "source": [
    "from rl2utils import QLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuX4yXnLfGP0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TabularQLearner (QLearner):\n",
    "    \"\"\" Tabular Q-learning agent. \"\"\"\n",
    "    # class level variables\n",
    "    learn_rate = 0.1    # learning rate of the Q-learning update\n",
    "    n_states = 40       # number of states per state-dimension\n",
    "    q_table = None      # table with Q-values\n",
    "    env_low = None\n",
    "    env_dx = None\n",
    "\n",
    "    def __init__(self, env, n_states=None):\n",
    "        \"\"\"Creates a tabular Q-learning agent with n_states in\n",
    "        each of the env(ironments) state directions.\n",
    "        \"\"\"\n",
    "        self.name = \"Tabular (%u states)\" % (n_states ** 2)\n",
    "        if n_states is not None:\n",
    "            self.n_states = n_states\n",
    "        self.env_low = env.observation_space.low\n",
    "        self.env_dx = (env.observation_space.high - self.env_low) / (self.n_states - 1)\n",
    "        self.q_table = np.zeros((self.n_states, self.n_states, env.action_space.n))\n",
    "\n",
    "    def _state_to_index(self, obs):\n",
    "        \"\"\" Maps an observed state to an index of the q_table. \"\"\"\n",
    "        a = int((obs[0] - self.env_low[0]) / self.env_dx[0])\n",
    "        b = int((obs[1] - self.env_low[1]) / self.env_dx[1])\n",
    "        return a, b\n",
    "\n",
    "    def q_values(self, state):\n",
    "        \"\"\" Returns the estimated Q-values (as a np.ndarray) of the given state. \"\"\"\n",
    "        a, b = self._state_to_index(state)\n",
    "        return self.q_table[a][b]\n",
    "\n",
    "    def sample(self, state):\n",
    "        \"\"\" Returns a greedily sampled action according to the estimated Q-values of the given state. \"\"\"\n",
    "        return np.argmax(self.q_values(state))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" The agent 'learns' from the given transition. \"\"\"\n",
    "        not_done = 0 if done else 1\n",
    "        a, b = self._state_to_index(state)\n",
    "        a_, b_ = self._state_to_index(next_state)\n",
    "        # Update Q-table with the TD-error for Q-learning (fill in to complete class)\n",
    "        #task\n",
    "        #task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QI4c8yNfGP2"
   },
   "source": [
    "## TASK 1b: the experiment \n",
    "We are going to import the open-ai gym loop function for intercation with the environment ```run_experiment``` and the plotting function ```plot_results```, that displays the experimental results in a unified fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1KgLbctfsV4"
   },
   "outputs": [],
   "source": [
    "from rl2utils import run_experiment\n",
    "\n",
    "from rl2utils import plot_all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GG1Cd6eCfGP8"
   },
   "source": [
    "## TASK 1c: test the tabular agent \n",
    "Run the above experiment with a TabularQLearner, which uses a discretization of 40 states per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcEizcYDfGP9"
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "PAxGSRM-fGP_",
    "outputId": "6c30b9f7-a98e-454b-ff64-4181ef085f44"
   },
   "outputs": [],
   "source": [
    "results.append(run_experiment(env, TabularQLearner(env, 40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "ROH75wBdfGQC",
    "outputId": "902147a4-0990-47cb-9886-7f01861354ee"
   },
   "outputs": [],
   "source": [
    "plot_all_results(results, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-j3ci6nQoRJn"
   },
   "source": [
    "***Pros***\n",
    "\n",
    "- Converges to a good policy on relatevely small state spaces\n",
    "- Stable\n",
    "- Intuitively easy to understand\n",
    "\n",
    "***Cons***\n",
    "\n",
    "- With the growth of the state space the Q-table grows also\n",
    "- For much more complex task/environements the small discretization is not enough to capture all the features of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QeXCdT6fGQF"
   },
   "source": [
    "# EXERCISE 2: Approximate Q-learning\n",
    "In this exercise you will implement a QLearner based on gradient descend with a linear Q-value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80A1lzRwfGQF"
   },
   "source": [
    "## TASK 2a: basis functions\n",
    "Now we want to use approximate Q-learning for linear functions. Familiarize yourself with our specification of basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T78W8ytB3hiK"
   },
   "source": [
    "We want to use some approximator to learn an action-value function $Q$:\n",
    "$$\\hat q(s,a,\\theta) \\approx q_{*}(s,a) \\space \\text{or} \\space q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTOoo8eV5mAw"
   },
   "source": [
    "Consider the objective function:\n",
    "$$L(\\theta) = \\alpha(R_{t+1} + \\gamma \\max_{a} \\hat{q}(s_{t+1},a,\\theta) - \\hat{q}(s_t,a_t,\\theta))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCAiuoy76-vo"
   },
   "source": [
    "The <b>semi-gradient</b> of this objective function will be:\n",
    "$$\\nabla \\theta_t = \\alpha(R_{t+1}+\\gamma \\max_{a} \\hat{q}(s_{t+1},a,\\theta) - \\hat{q}(s_t,a_t,\\theta))\\frac{\\partial \\hat{q}(s_t, a_t, \\theta_t)}{\\partial \\theta_t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXS6vCuUiOkE"
   },
   "outputs": [],
   "source": [
    "from rl2utils import BasisFunctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4qMam-tfGQJ"
   },
   "outputs": [],
   "source": [
    "class OneHot (BasisFunctions):\n",
    "    \"\"\" one-hot encoding of 2d-spaces \"\"\"\n",
    "    n_states = None\n",
    "\n",
    "    def __init__(self, env, n_states):\n",
    "        \"\"\" Creates a one-hot encoding with n_states discrete intervals in each input dimension. \"\"\"\n",
    "        BasisFunctions.__init__(self, env)\n",
    "        self.name = \"OneHot\"\n",
    "        self.n_states = n_states\n",
    "        self.num_features = n_states ** 2\n",
    "\n",
    "    def __call__(self, state):\n",
    "        \"\"\" Overrides the () operator and returns a one-hot encoding of the given state.\"\"\"\n",
    "        phi = np.zeros(self.num_features)\n",
    "        index = np.floor((state - self._env_low) / self._env_dx * (self.n_states-1))\n",
    "        phi[int(index[0] * self.n_states + index[1])] = 1\n",
    "        return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Anbmel-fGQL"
   },
   "outputs": [],
   "source": [
    "class RBF (BasisFunctions):\n",
    "    \"\"\" Exponential radial basis functions in 2d spaces.\"\"\"\n",
    "    normalize = True      # this flag normalizes the L_1 norm of the output vector to 1\n",
    "    centers = None\n",
    "    sigmas = None\n",
    "\n",
    "    def __init__(self, env, n_bases):\n",
    "        \"\"\" Creates a set of equidistant basis functions with n_bases functions in each input dimension. \"\"\"\n",
    "        BasisFunctions.__init__(self, env)\n",
    "        self.name = \"RBF\"\n",
    "        self.num_features = n_bases ** 2\n",
    "        self._make_centers(n_bases)\n",
    "    \n",
    "    def _make_centers(self, n_bases):\n",
    "        \"\"\" Initializes the centers of the RBF basis functions as an equidistant grid \n",
    "            and width sigma as the distance between the centers to guarantee enough overlap. \"\"\"\n",
    "        # Create centers of the RBF\n",
    "        self.centers = np.zeros((2, n_bases ** 2))\n",
    "        self.centers[0, :] = np.repeat(np.linspace(self._env_low[0], self._env_high[0], num=n_bases), n_bases)\n",
    "        self.centers[1, :] = np.tile(np.linspace(self._env_low[1], self._env_high[1], num=n_bases), n_bases)\n",
    "        # Create widths of the RBF\n",
    "        self.sigmas = np.zeros((2, n_bases ** 2))\n",
    "        self.sigmas[0, :] = np.repeat(np.ones(n_bases) * self._env_dx[0] / (n_bases-1), n_bases)\n",
    "        self.sigmas[1, :] = np.tile(np.ones(n_bases)  * self._env_dx[1] / (n_bases-1), n_bases)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        \"\"\" Overrides the () operator and returns the (normalised) RBF output for the given state.\"\"\"\n",
    "        # Compute the RBFs\n",
    "        phi = np.exp(- np.sum(((self.centers - np.expand_dims(state, axis=1)) / self.sigmas) ** 2, axis=0))\n",
    "        # Optionally normalize the RBF output to sum 1\n",
    "        if self.normalize:\n",
    "            phi /= np.sum(phi, axis=0)\n",
    "        # Return feature(s)\n",
    "        return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIv43cRJfGQM"
   },
   "source": [
    "## Task 2b: semi-gradients \n",
    "Derive the semi-gradient of the quadratic Bellman-error of Q-learning at time $t$ (i.e. after observing s_t, a_t, r_t and s_{t+1}) for a linear Q-function. Let us write down both the loss function and the semi-gradient as a LaTeX formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cy2eou6V9Dcu"
   },
   "source": [
    "Let $\\phi(s) = (\\phi_1(s), \\phi_2(s) ... \\phi_d(s))$ be our feature vector (defined by our basis function), so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulbhU4DwfGQN"
   },
   "source": [
    "\n",
    "The Q-value function is given by $Q(s,a) = \\vec w^{a} \\!\\!\\cdot \\vec \\phi(s)$.\n",
    "\\begin{eqnarray*} \n",
    "    \\mathcal{L}_t &=& {\\textstyle\\frac{1}{2}} \\Big( r_t \n",
    "                            + \\gamma \\max_{a'}\\!\\big\\{ Q(s_{t+1}, a') \\big\\} \n",
    "                            - Q(s_t, a_t) \\Big)^2\n",
    "                    \\;=\\; {\\textstyle\\frac{1}{2}} \\Big( r_t \n",
    "                            + \\gamma \\max_{a'}\\!\\big\\{\\vec w^{a'} \\!\\!\\cdot \\vec \\phi(s_{t+1}) \\big\\} \n",
    "                            - \\vec w^{a_t} \\!\\!\\cdot \\vec \\phi(s_t) \\Big)^2\n",
    "\\\\[2mm]\n",
    "    \\frac{\\partial \\mathcal{L}_t}{\\partial \\vec w^{a}}  &=& \n",
    "            \\left\\{ \\begin{array}{ll}\n",
    "                                \\big( r_t \n",
    "                                  + \\gamma \\max_{a'}\\!\\big\\{ Q(s_{t+1}, a') \\big\\} \n",
    "                                  - Q(s_t, a_t) \\big) \\; \\vec \\phi(s_t)\n",
    "                            & \\text{if }a = a_t \\\\\n",
    "                            \\vec 0  &\\text{if }a \\neq a_t \n",
    "            \\end{array}\\right.\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vKDvRV18pHL"
   },
   "source": [
    ">[Shimon Whiteson lecture 2, p.7](https://github.com/mlss-skoltech/lectures/blob/master/reinforcement_learning/lecture2_fa-2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-MJV4qofGQO"
   },
   "source": [
    "## TASK 2c: linear Q-functions \n",
    "Implement the corresponding semi-gradient QLearner in the below skeleton. Make sure that, similarly to task 1a, the end of the episode (done=True) is handled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuvZwOyWfGQO"
   },
   "outputs": [],
   "source": [
    "class LinearQLearner (QLearner):\n",
    "    learn_rate = 0.1\n",
    "    basis = None\n",
    "    weights = None\n",
    "\n",
    "    def __init__(self, env, basis: BasisFunctions):\n",
    "        self.name = \"Linear (%u %s)\" % (basis.num_features, basis.name)\n",
    "        self.basis = basis\n",
    "        self.weights = np.stack([basis.new_weights() for _ in range(env.action_space.n)], axis=0)\n",
    "\n",
    "    def q_values(self, state):\n",
    "        \"\"\" Returns the estimated Q-values (as a np.ndarray) of the given state. \"\"\"\n",
    "        return self.weights.dot(self.basis(state))\n",
    "\n",
    "    def sample(self, state):\n",
    "        \"\"\" Returns an action the agent has chosen at the given state. \"\"\"\n",
    "        return np.argmax(self.q_values(state))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" The agent 'learns' from the given transition. \"\"\"\n",
    "        not_done = 0 if done else 1\n",
    "        #task\n",
    "        #task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtgISUHOfGQQ"
   },
   "source": [
    "## TASK 2d: approximation with one-hot bases \n",
    "Test your above implementation with one-hot basis functions (with 40 bases in each state-dimension). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "oU_UgTD_fGQQ",
    "outputId": "6a583125-8951-411e-838b-d17486c68fee"
   },
   "outputs": [],
   "source": [
    "results.append(run_experiment(env, LinearQLearner(env, OneHot(env, 40))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "lZZvM_zDfGQS",
    "outputId": "3ad2cd6c-8bf9-40c5-8aac-30a1a8ca28f7"
   },
   "outputs": [],
   "source": [
    "plot_all_results(results, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kk0GmpY7fGQU"
   },
   "source": [
    "Gradient descent on one-hot vectors are equivalent to tabular Q-learning. Do your results mirror the performance of the TabularQLearner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9BAwVIbfGQV"
   },
   "source": [
    "## Task 2e: approximation with RBF bases\n",
    "Now test your LinearQLearner with the given RBF bases (15 centers for each state dimension). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "colab_type": "code",
    "id": "O6yL1SAsfGQV",
    "outputId": "432143fc-bcec-49ce-98d1-257a82beffb3"
   },
   "outputs": [],
   "source": [
    "results.append(run_experiment(env, LinearQLearner(env, RBF(env, 16))))\n",
    "plot_all_results(results, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntChPYKofGQY"
   },
   "source": [
    "Function approximation allows the agent to learn much faster, but the agent's performance can become extremly unstable in later episodes. This is somewhat surprising, as the lecture has established that semi-gradient TD(0) learning is supposed to converge in the linear case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYAJ2pTvfGQY"
   },
   "source": [
    "# EXERCISE 3: Stabilising Approximate Q-learning\n",
    "Q-learning with semi-gradient descent is often unstable. In this exercise you will implement techniques that have emerged to stabilise approximate RL, namely Target Networks and Experience Replay Buffers.\n",
    "\n",
    "## Task 3a: target networks\n",
    "In this exercise you will improve the stability of gradient descend by using target networks. Note that for our purposes, the term \"target network\" refers to another weight vector. Extend the LinearQLearner with a target network, which is periodically updated with the current weights every 100 gradient steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYz8iC_XfGQZ"
   },
   "outputs": [],
   "source": [
    "class TargetQLearner (LinearQLearner):\n",
    "    target_weights = None\n",
    "    num_updates = 0\n",
    "    update_target = 100\n",
    "\n",
    "    def __init__(self, env, basis):\n",
    "        LinearQLearner.__init__(self, env, basis)\n",
    "        self.name = \"Target (%u %s)\" % (basis.num_features, basis.name)\n",
    "        self.target_weights = self.weights.copy()\n",
    "\n",
    "    def target_q_values(self, state):\n",
    "        \"\"\" Returns the estimated Q-values (as a np.ndarray) of the given state. \"\"\"\n",
    "        return self.target_weights.dot(self.basis(state))\n",
    "\n",
    "    def _refresh_target(self):\n",
    "        \"\"\" Refresh the target network every update_target steps \"\"\"\n",
    "        self.num_updates += 1\n",
    "        if self.num_updates % self.update_target == 0:\n",
    "            self.target_weights = self.weights.copy()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self._refresh_target()\n",
    "        # Compute TD error\n",
    "        not_done = 0 if done else 1\n",
    "        #task\n",
    "        # Gradient descend step\n",
    "        self.weights[action, :] += self.learn_rate * td_error * self.basis(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmFzfe7XfGQb"
   },
   "source": [
    "Now test your above TargetQLearner with the same 16x16 RBF bases as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "colab_type": "code",
    "id": "LevKAoT5fGQc",
    "outputId": "b1a14020-2157-4896-cf61-ec55819a3c52"
   },
   "outputs": [],
   "source": [
    "results.append(run_experiment(env, TargetQLearner(env, RBF(env, 16))))\n",
    "plot_all_results(results, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GydJa-ULfGQf"
   },
   "source": [
    "The target networks can stabilise an agent's performance quite a bit, but do not have to. Do you observe more stable learning? Is the behaviour still unstable at the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6SIh7u-fGQf"
   },
   "source": [
    "## TASK 3b: double Q-learning\n",
    "Extend the class TargetQLearner to implement double Q-learning (akin to Double DQN from the lecture).\n",
    ">[Shimon Whiteson lecture 2, p. 28](https://github.com/mlss-skoltech/lectures/blob/master/reinforcement_learning/lecture2_fa-2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1LM3wc8vkJZ"
   },
   "source": [
    "Consider an MDP having four states two of which are terminal states.\n",
    "State A is always considered at start state, and has two actions, either Right or Left. The Right action gives zero reward and lands in terminal state C.\n",
    "The Left action moves the agent to state B with zero reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Jhz-UbUADdE"
   },
   "source": [
    "<img src='https://github.com/NinaMaz/mlss-tutorials/blob/master/whiteson/img/double_q.png?raw=1'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOG4OzRUAf0S"
   },
   "source": [
    "State B has a number of actions, they move the agent to the terminal state D. However (this is important) the reward R of each action from B to D has a random value that follows a normal distribution with mean -0.5 and a variance 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WHLSr5qBtkj"
   },
   "source": [
    "<img src=\"https://github.com/NinaMaz/mlss-tutorials/blob/master/whiteson/img/double_q_alg.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfzgkVIefGQg"
   },
   "outputs": [],
   "source": [
    "class DoubleQLearner (TargetQLearner):\n",
    "    \n",
    "    def __init__(self, env, basis):\n",
    "        TargetQLearner.__init__(self, env, basis)\n",
    "        self.name = \"Double (%u %s)\" % (basis.num_features, basis.name)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" The agent 'learns' from the given transition. \"\"\"\n",
    "        self._refresh_target()\n",
    "        # Compute greedy actions at next_state with the current Q-function\n",
    "        greedy = np.argmax(self.q_values(next_state)) #task\n",
    "        # Compute TD-error with greedy actions for the target Q-function\n",
    "        not_done = 0 if done else 1\n",
    "        td_error = reward + not_done*self.gamma*self.target_q_values(next_state)[greedy] \\\n",
    "                          - self.q_values(state)[action]#task\n",
    "        # Gradient descend step\n",
    "        self.weights[action, :] += self.learn_rate * td_error * self.basis(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qh9smfeufGQi"
   },
   "source": [
    "Now test your above DoubleQLearner with the same RBF bases as above. Double Q-learning should further stabilise the agent's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kaq5DMcXfGQj",
    "outputId": "f3fa03ab-0a3e-4931-e491-e2989be4d39f"
   },
   "outputs": [],
   "source": [
    "results.append(run_experiment(env, DoubleQLearner(env, RBF(env, 16))))\n",
    "plot_all_results(results, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tT9iT3OVfGQl"
   },
   "source": [
    "If you do not have a stable solution by now, increase the number of basis fuctions for each state dimension.\n",
    "How does the policy look like in comparison to the tabular case in task 1c? Can you formulate an optimal policy for mountain-car based on your observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOWY2EvkfGQl"
   },
   "source": [
    "# EXERCISE 4: Deep Q-Networks\n",
    "In this exercise we will implement Q-learning with a neural network. Read the DQN paper by Mnih et al. https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf for help with the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWMoo7mVfGQm"
   },
   "source": [
    "## TASK 4a\n",
    "Implement a deep Q net that holds 10,000 transitions and uses a mini-batch of 32 samples. Make sure that the mini-batch always includes the newest transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzY-jQJYfGQm"
   },
   "source": [
    "The following code describes a simple replay buffer which is used to store (state, action, reward, next_state) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MaU45NhcfGQn"
   },
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        for i in range(capacity):\n",
    "                self.buffer.append(None)\n",
    "        self.capacity = capacity\n",
    "        self.insert_index = 0\n",
    "        self.num_exp = 0\n",
    "\n",
    "    def push(self, sarst):\n",
    "        self.buffer[self.insert_index] = sarst\n",
    "        self.insert_index = (self.insert_index + 1)%self.capacity\n",
    "        if self.num_exp < self.capacity:\n",
    "            self.num_exp += 1\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        index = np.random.randint(self.num_exp-1)\n",
    "        return self.buffer[index]\n",
    "\n",
    "    def sample_batch(self, size):\n",
    "        batch = []\n",
    "        for i in range(size):\n",
    "            batch.append(self.sample_buffer())\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2DOdvxZfGQp"
   },
   "source": [
    "Similarly the code-let below describes simple a 3 layer neural network which outputs an action value for all the valid actions in mountaincar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfC82k96fGQq"
   },
   "outputs": [],
   "source": [
    "class Value_Net(torch.nn.Module):\n",
    "    hidden = 20\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Value_Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(2, self.hidden)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden, self.hidden)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcuNmwFnfGQs"
   },
   "source": [
    "Use the previously defined classes for replay buffer and value network to implement your DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE731RD5fGQs"
   },
   "outputs": [],
   "source": [
    "class DQLearner(QLearner):\n",
    "    buffer_size = 50000\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-4  # 1e-3\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"DQN\"\n",
    "        self.epsilon = 1.0\n",
    "        self.qnet = Value_Net()\n",
    "        self.buffer = Replay_buffer(self.buffer_size)\n",
    "        #self.optimizer = torch.optim.RMSprop(self.qnet.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer = torch.optim.RMSprop(self.qnet.parameters())\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def q_values(self, state):\n",
    "        return self.qnet(torch.Tensor(state))\n",
    "\n",
    "    def sample(self, state):\n",
    "        return np.argmax(self.q_values(state).detach().numpy())\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        obs = {'old': state, 'action': action, 'reward': reward, 'new': next_state, 'done': done}\n",
    "        self.buffer.push(obs)\n",
    "        if self.buffer.num_exp < self.batch_size:\n",
    "            return\n",
    "        else:\n",
    "            batch = self.buffer.sample_batch(self.batch_size - 1)\n",
    "            batch.append(obs)\n",
    "            states = torch.Tensor(list(map(lambda x: x['old'], batch)))\n",
    "            actions = torch.LongTensor(list(map(lambda x: x['action'], batch))).unsqueeze(1)\n",
    "            rewards = torch.Tensor(list(map(lambda x: x['reward'], batch)))\n",
    "            vals = self.qnet(states).gather(1, actions)\n",
    "            vals = torch.squeeze(vals)\n",
    "            targets = torch.zeros(self.batch_size)\n",
    "            non_terminal_mask = torch.ByteTensor(list(map(lambda x: not x['done'], batch)))\n",
    "            next_states_nont = torch.Tensor([s['new'] for s in batch if not s['done']])\n",
    "            targets[non_terminal_mask] = self.qnet(next_states_nont).max(1)[0].detach()\n",
    "            targets = targets * self.gamma + rewards\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(vals, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            return\n",
    "    \n",
    "    def set_epsilon(self, iter):\n",
    "        \"\"\" Exponentially decays the eploration parameter epsilon. \"\"\"\n",
    "        self.epsilon = nate**(-iter/250.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2j6kiMIQfGQu",
    "outputId": "628f3c53-8a49-4caf-b713-f03522edc6cb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.append(run_experiment(env, DQLearner()))\n",
    "plot_all_results(results, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQ4OGtbkfGQv"
   },
   "source": [
    "# EXERCISE 5: Improve Q-learning\n",
    "If you have finished the above tasks and still have some time left, play aound with your solutions to improve performance. In particular, test the following changes for any of the above classes that inherit from LinearQLearner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3DFvTqBfGQw"
   },
   "source": [
    "## TASK 5a: number of basis function\n",
    "Plot the performance of the LinearQLearner (and it's descendants) against the number of used RBF basis functions in [10x10, ..., 20x20]. Does an increasing number of bases increase or decrease stability? Note that you need to write your own plotting script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LL2-UhAHfGQw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycas2qHJfGQy"
   },
   "source": [
    "## TASK 5b: learning rate\n",
    "Change the implementation of LinearQLearner (and it's descendants) to decrease the learning rate slowly. How does this change affect learning rate and stability? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfsocU_GfGQz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvHrSc0afGQ2"
   },
   "source": [
    "## TASK 5c: exploration\n",
    "The experiment defined in task 1b uses epsilon-greedy exploration with a constant epsilon=0.1. Override the set_epsilon method of (a subclass of) LinearQLearner to implement a decay shedule, which decreases epsilon linearly from 1 to 0.01 in {10, 50, 100, 500} epsiodes. Does this improve the performance or not? What happens with an exponential decay schedule as in DQLearner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3YlmzlDfGQ3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be1I8BTAfGQ5"
   },
   "source": [
    "## TASK 5d: DQN variations\n",
    "There are several things that can be tried out with the DQN. Figure out if using different learning rates, losses and network architecture (number of layers and connectivity) changes the network performance. You can even change the replay buffer implementation to define some heuristic to draw samples from. Explore if these modifications help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aX_wLTqgfGQ6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXwJzdbffGQ7"
   },
   "source": [
    "## TASK 5e: scientific evaluation\n",
    "The experiment defined in task 1b fixes the random seed to ensure reproducability of results. However, each algorithm can behave very different depending on the seed. Run one (or all) above experiment(s) with 10 random seeds (i.e. comment out the lines that set the seed to 0) and plot the mean and staqndard deviation of the received rewards. Note that you may want to write your own plotting script, in which the shaded area refers to another type of standard deviation (between seeds, not over time) than in the given script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wAVIz9kfGQ7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8_K3TrCCRbt"
   },
   "source": [
    "## [cut?] TASK 3c: experience replay buffers\n",
    "In this exercise you will further stabilize gradient descent by introducing an experience replay buffer.\n",
    "Extend the above TargetQLearner with an experience replay buffer, that holds 10,000 transitions and uses a mini-batch of 32 samples. Make sure that the mini-batch always includes the newest transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jH6Ww9BlCQfh"
   },
   "outputs": [],
   "source": [
    "class ReplayQLearner(DoubleQLearner):\n",
    "    buffer_size = 20000\n",
    "    mini_batch = 32\n",
    "    buffer = None\n",
    "\n",
    "    def __init__(self, env, basis):\n",
    "        super().__init__(env, basis)\n",
    "        self.name = \"Replay (%u %s)\" % (basis.num_features, basis.name)\n",
    "        self.buffer = []\n",
    "        # Adjust inherited parameters to work with the higher update rate\n",
    "        self.learn_rate = self.learn_rate / self.mini_batch\n",
    "        self.update_target = self.update_target * self.mini_batch\n",
    "\n",
    "    def insert(self, state, action, reward, next_state, done):\n",
    "        # Insert the observed transition into the replay buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        # Always add (at least) the observed transition to the mini-batch\n",
    "        batch = [(state, action, reward, next_state, done)]\n",
    "        # Add mini_batch-1 randomly drawn past transitions to the mini-batch\n",
    "        for _ in range(min(self.mini_batch, len(self.buffer)) - 1):\n",
    "            # Draw a random transition from the last buffer_size entries of the replay buffer\n",
    "            i = np.random.randint(max(0, len(self.buffer) - self.buffer_size), len(self.buffer))\n",
    "            batch.append(self.buffer[i])\n",
    "        return batch\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        batch = self.insert(state, action, reward, next_state, done)\n",
    "        for (s, a, r, ns, d) in batch:\n",
    "            super().update(s, a, r, ns, d)\n",
    "            \n",
    "    def set_epsilon(self, iter):\n",
    "        \"\"\" Maybe we must be random until the buffer is full? \"\"\"\n",
    "        self.epsilon = nate**(-iter/250.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0dHZL95fGRA"
   },
   "source": [
    "Now test your ReplayQLearner with the same RBF bases as above. \n",
    "\n",
    "*Hint:* depending on the implementation, this experiment may run much longer than those before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAdOrR0hfGRA",
    "outputId": "abb68871-8aec-4d58-d8cd-e90aadf417cc"
   },
   "outputs": [],
   "source": [
    "results.append(run_experiment(env, ReplayQLearner(env, RBF(env, 16))))\n",
    "plot_all_results(results, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OGAcNmWfGRE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NC_xFKofGRJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SI0o8V25fGRK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "day2_solutions_new.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
