{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Network\n",
    "\n",
    "\\[[paper](https://arxiv.org/abs/1710.10903)\\] , \\[[original code](https://github.com/PetarV-/GAT)\\] , \\[[all other implementations](https://paperswithcode.com/paper/graph-attention-networks)\\]\n",
    "\n",
    "From [Graph Convolutional Network (GCN)](https://arxiv.org/abs/1609.02907), we learned that combining local graph structure and node-level features yields good performance on node classification task. Hwever, the way GCN aggregates is structure-dependent, which may hurt its generalizability.\n",
    "\n",
    "One workaround is to simply average over all neighbor node features as in GraphSAGE. Graph Attention Network proposes an alternative way by weighting neighbor features with feature dependent and structure free normalization, in the style of attention.\n",
    "\n",
    "The goal of this tutorial:\n",
    "\n",
    "- Explain what is Graph Attention Network.\n",
    "- Understand the attentions learnt.\n",
    "- Introduce to inductive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing Attention to GCN\n",
    "----------------------------\n",
    "\n",
    "The key difference between GAT and GCN is how the information from the one-hop neighborhood is aggregated.\n",
    "\n",
    "For GCN, a graph convolution operation produces the normalized sum of the node features of neighbors:\n",
    "\n",
    "\n",
    "$$h_i^{(l+1)}=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\frac{1}{c_{ij}} W^{(l)}h^{(l)}_j}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathcal{N}(i)$ is the set of its one-hop neighbors (to include $v_i$ in the set, simply add a self-loop to each node),\n",
    "$c_{ij}=\\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$ is a normalization constant based on graph structure, $\\sigma$ is an activation function (GCN uses ReLU), and $W^{(l)}$ is a shared weight matrix for node-wise feature transformation. Another model proposed in\n",
    "[GraphSAGE](https://www-cs-faculty.stanford.edu/people/jure/pubs/graphsage-nips17.pdf)\n",
    "employs the same update rule except that they set\n",
    "$c_{ij}=|\\mathcal{N}(i)|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT introduces the attention mechanism as a substitute for the statically\n",
    "normalized convolution operation. Below are the equations to compute the node\n",
    "embedding $h_i^{(l+1)}$ of layer $l+1$ from the embeddings of\n",
    "layer $l$:\n",
    "\n",
    "<img src=\"https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/gat/gat.png\" height=\"350\" width=\"450\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade git+https://github.com/mlss-skoltech/tutorials_week2.git#subdirectory=graph_neural_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /anaconda3/lib/python3.7/site-packages/gnnutils/data/data.zip\n",
      "  inflating: ./data/ind.cora.ally    \n",
      "  inflating: ./data/ind.cora.test.index  \n",
      "  inflating: ./data/ind.pubmed.graph  \n",
      "  inflating: ./data/ind.cora.allx    \n",
      "  inflating: ./data/ind.citeseer.test.index  \n",
      "  inflating: ./data/ind.citeseer.ty  \n",
      "  inflating: ./data/ind.citeseer.tx  \n",
      "  inflating: ./data/ind.citeseer.graph  \n",
      "  inflating: ./data/preprocessed_MNIST.dump  \n",
      "  inflating: ./data/ind.cora.tx      \n",
      "  inflating: ./data/ind.pubmed.test.index  \n",
      "  inflating: ./data/ind.cora.ty      \n",
      "  inflating: ./data/ind.cora.graph   \n",
      "  inflating: ./data/ind.citeseer.y   \n",
      "  inflating: ./data/ind.citeseer.ally  \n",
      "  inflating: ./data/ind.citeseer.allx  \n",
      "  inflating: ./data/ind.citeseer.x   \n",
      "  inflating: ./data/ind.pubmed.ally  \n",
      "  inflating: ./data/ind.pubmed.allx  \n",
      "  inflating: ./data/ind.pubmed.x     \n",
      "  inflating: ./data/ind.pubmed.tx    \n",
      "  inflating: ./data/ind.cora.x       \n",
      "  inflating: ./data/ind.pubmed.ty    \n",
      "  inflating: ./data/ind.pubmed.y     \n",
      "  inflating: ./data/ind.cora.y       \n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "ZIP_PATH = pkg_resources.resource_filename('gnnutils', 'data/data.zip')\n",
    "DATA_PATH = './data'\n",
    "\n",
    "!unzip -u {ZIP_PATH} -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.sparse.linalg as la\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "from gnnutils import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_no_weights():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print('#weights in the model: %d' % (total_parameters,))\n",
    "\n",
    "def frobenius_norm(tensor):\n",
    "    square_tensor = tf.square(tensor)\n",
    "    tensor_sum = tf.reduce_sum(square_tensor)\n",
    "    frobenius_norm = tf.sqrt(tensor_sum)\n",
    "    return frobenius_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT:\n",
    "    \n",
    "    \"\"\"\n",
    "    The neural network model.\n",
    "    \"\"\"\n",
    "    def __init__(self, idx_rows, idx_cols, A_shape, X, Y, num_hidden_feat, n_heads, learning_rate=5e-2, gamma=1e-3, idx_gpu = '/gpu:3'):\n",
    "        \n",
    "        self.num_hidden_feat = num_hidden_feat\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma=gamma\n",
    "        with tf.Graph().as_default() as g:\n",
    "                self.graph = g\n",
    "                \n",
    "                with tf.device(idx_gpu):\n",
    "                            \n",
    "                        # list of weights' tensors l2-loss \n",
    "                        self.regularizers = []\n",
    "                            \n",
    "                        #definition of constant matrices\n",
    "                        self.X = tf.constant(X, dtype=tf.float32) \n",
    "                        self.Y = tf.constant(Y, dtype=tf.float32)\n",
    "                        \n",
    "                        #placeholder definition\n",
    "                        self.idx_nodes = tf.placeholder(tf.int32)\n",
    "                        self.keep_prob = tf.placeholder(tf.float32)\n",
    "                        \n",
    "                        #model definition\n",
    "                        \n",
    "                        self.X0 = []\n",
    "                        for k in range(n_heads):\n",
    "                            with tf.variable_scope('GCL_1_{}'.format(k+1)):\n",
    "                                self.X0.append(self.GAT_layer(self.X, num_hidden_feat, idx_rows, idx_cols, A_shape, tf.nn.elu))\n",
    "                        self.X0 = tf.concat(self.X0, 1)\n",
    "                        \n",
    "                        with tf.variable_scope('GCL_2'):\n",
    "                            self.logits = self.GAT_layer(self.X0, Y.shape[1], idx_rows, idx_cols, A_shape, tf.identity)\n",
    "                        \n",
    "                        self.l_out = tf.gather(self.logits, self.idx_nodes)\n",
    "                        self.c_Y = tf.gather(self.Y, self.idx_nodes)\n",
    "                        \n",
    "                        #loss function definition\n",
    "                        self.l2_reg = tf.reduce_sum(self.regularizers)\n",
    "                        self.data_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.l_out, labels=self.c_Y)) \n",
    "                        \n",
    "                        self.loss = self.data_loss + self.gamma*self.l2_reg\n",
    "                        \n",
    "                        #solver definition\n",
    "                        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "                        self.opt_step = self.optimizer.minimize(self.loss)\n",
    "                        \n",
    "                        #predictions and accuracy extraction\n",
    "                        self.c_predictions = tf.argmax(tf.nn.softmax(self.l_out), 1)\n",
    "                        self.accuracy = tf.contrib.metrics.accuracy(self.c_predictions, tf.argmax(self.c_Y, 1))\n",
    "                        \n",
    "                        #gradients computation\n",
    "                        self.trainable_variables = tf.trainable_variables()\n",
    "                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n",
    "                        self.norm_grad = frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n",
    "                        \n",
    "                        #session creation\n",
    "                        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "                        config.gpu_options.allow_growth = True\n",
    "                        self.session = tf.Session(config=config)\n",
    "\n",
    "                        #session initialization\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        self.session.run(init)\n",
    "                        \n",
    "                        count_no_weights()\n",
    "\n",
    "    def GAT_layer(self, X, Fout, idx_rows, idx_cols, A_shape, activation):\n",
    "        X = tf.nn.dropout(X,  self.keep_prob)\n",
    "        \n",
    "        W = tf.get_variable(\"W\", shape=[X.shape[1], Fout], initializer=tf.glorot_uniform_initializer())\n",
    "        self.regularizers.append(tf.nn.l2_loss(W))\n",
    "        X_w = tf.matmul(X, W)\n",
    "\n",
    "        # simplest possible attention mechanism\n",
    "        W_att1 = tf.get_variable(\"W_att1\", shape=[X_w.shape[1], 1], initializer=tf.glorot_uniform_initializer())\n",
    "        b_att1 = tf.get_variable(\"b_att1\", shape=[1,], initializer=tf.zeros_initializer())\n",
    "        self.regularizers.append(tf.nn.l2_loss(W_att1))\n",
    "        W_att2 = tf.get_variable(\"W_att2\", shape=[X_w.shape[1], 1], initializer=tf.glorot_uniform_initializer())\n",
    "        b_att2 = tf.get_variable(\"b_att2\", shape=[1,], initializer=tf.zeros_initializer())\n",
    "        self.regularizers.append(tf.nn.l2_loss(W_att2))\n",
    "                            \n",
    "        X_att_1 = tf.squeeze(tf.matmul(X_w, W_att1)) + b_att1\n",
    "        X_att_2 = tf.squeeze(tf.matmul(X_w, W_att2)) + b_att2\n",
    "        \n",
    "        logits = tf.gather(X_att_1, idx_rows) +  tf.gather(X_att_2, idx_cols)\n",
    "                            \n",
    "        A_att = tf.SparseTensor(indices=np.vstack([idx_rows, idx_cols]).T, \n",
    "                                values=tf.nn.leaky_relu(logits), \n",
    "                                dense_shape=A_shape)\n",
    "        A_att = tf.sparse_reorder(A_att)\n",
    "        A_att = tf.sparse_softmax(A_att)\n",
    "        \n",
    "        # apply dropout\n",
    "        A_att = tf.SparseTensor(indices=A_att.indices,\n",
    "                                values=tf.nn.dropout(A_att.values, self.keep_prob),\n",
    "                                dense_shape=A_shape)\n",
    "        A_att = tf.sparse_reorder(A_att)\n",
    "\n",
    "        X_w = tf.nn.dropout(X_w, self.keep_prob)\n",
    "        res = tf.sparse_tensor_dense_matmul(A_att, X_w)\n",
    "        res = tf.contrib.layers.bias_add(res)\n",
    "\n",
    "        return activation(res)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head Attention\n",
    "^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Analogous to multiple channels in ConvNet, GAT introduces **multi-head\n",
    "attention** to enrich the model capacity and to stabilize the learning\n",
    "process. Each attention head has its own parameters and their outputs can be\n",
    "merged in two ways:\n",
    "\n",
    "\\begin{align}\\text{concatenation}: h^{(l+1)}_{i} =||_{k=1}^{K}\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)\\end{align}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{align}\\text{average}: h_{i}^{(l+1)}=\\sigma\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j\\in\\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)\\end{align}\n",
    "\n",
    "where $K$ is the number of heads. The authors suggest using\n",
    "concatenation for intermediary layers and average for the final layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 2708)\n",
      "(2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "#learning parameters and path dataset\n",
    "\n",
    "learning_rate = 5e-3\n",
    "val_test_interval = 1\n",
    "num_hidden_feat = 8\n",
    "n_heads = 8\n",
    "gamma = 5e-4\n",
    "patience = 100\n",
    "path_dataset = './CORA/dataset.pickle'\n",
    "    \n",
    "#dataset loading\n",
    "#ds = Dataset(path_dataset, normalize_feat=1)\n",
    "\n",
    "A, X, Y, train_idx, val_idx, test_idx = process_data.load_data(\"cora\", path_to_data=DATA_PATH)\n",
    "X = process_data.preprocess_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts rows and cols of adjacency matrix\n",
    "A = sp.csr_matrix(A)\n",
    "A.setdiag(1)\n",
    "\n",
    "idx_rows, idx_cols = A.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_exp = 10 #number of times training GCN over the given dataset\n",
    "num_exp = 1 #number of times training GCN over the given dataset\n",
    "\n",
    "list_all_acc = []\n",
    "list_all_cost_val_avg  = []\n",
    "list_all_data_cost_val_avg = []\n",
    "list_all_acc_val_avg   = []\n",
    "list_all_cost_test_avg = []\n",
    "list_all_acc_test_avg  = []\n",
    "\n",
    "num_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_total_iter_training = int(10e4)\n",
    "\n",
    "GCNN = GAT(idx_rows, idx_cols, A.shape, X, Y, num_hidden_feat, n_heads, learning_rate=learning_rate, gamma=gamma)\n",
    "\n",
    "cost_train_avg      = []\n",
    "grad_norm_train_avg = []\n",
    "acc_train_avg       = []\n",
    "cost_test_avg       = []\n",
    "grad_norm_test_avg  = []\n",
    "acc_test_avg        = []\n",
    "cost_val_avg        = []\n",
    "data_cost_val_avg   = []\n",
    "acc_val_avg         = []\n",
    "iter_test           = []\n",
    "list_training_time = list()\n",
    "\n",
    "max_val_acc = 0\n",
    "min_val_loss = np.inf\n",
    "\n",
    "#Training code\n",
    "for i in tqdm(range(num_total_iter_training)):\n",
    "    if (len(cost_train_avg) % val_test_interval) == 0:\n",
    "        #Print last training performance\n",
    "        if (len(cost_train_avg)>0):\n",
    "            tqdm.write(\"[TRN] epoch = %03i, cost = %3.2e, |grad| = %.2e, acc = %3.2e (%03.2fs)\" % \\\n",
    "            (len(cost_train_avg), cost_train_avg[-1], grad_norm_train_avg[-1], acc_train_avg[-1], time.time() - tic))\n",
    "\n",
    "        #Validate the model\n",
    "        tic = time.time()\n",
    "\n",
    "        feed_dict = {GCNN.idx_nodes: val_idx, GCNN.keep_prob:1.0}\n",
    "        acc_val, cost_val, data_cost_val = GCNN.session.run([GCNN.accuracy, GCNN.loss, GCNN.data_loss], feed_dict)\n",
    "\n",
    "        data_cost_val_avg.append(data_cost_val)\n",
    "        cost_val_avg.append(cost_val)\n",
    "        acc_val_avg.append(acc_val)\n",
    "        tqdm.write(\"[VAL] epoch = %03i, data_cost = %3.2e, cost = %3.2e, acc = %3.2e (%03.2fs)\" % \\\n",
    "            (len(cost_train_avg), data_cost_val_avg[-1], cost_val_avg[-1], acc_val_avg[-1],  time.time() - tic))\n",
    "\n",
    "        #Test the model\n",
    "        tic = time.time()\n",
    "\n",
    "        feed_dict = {GCNN.idx_nodes: test_idx, GCNN.keep_prob:1.0}\n",
    "        acc_test, cost_test = GCNN.session.run([GCNN.accuracy, GCNN.loss], feed_dict)\n",
    "\n",
    "        cost_test_avg.append(cost_test)\n",
    "        acc_test_avg.append(acc_test)\n",
    "        tqdm.write(\"[TST] epoch = %03i, cost = %3.2e, acc = %3.2e (%03.2fs)\" % \\\n",
    "            (len(cost_train_avg), cost_test_avg[-1], acc_test_avg[-1],  time.time() - tic))\n",
    "        iter_test.append(len(cost_train_avg))\n",
    "\n",
    "\n",
    "        if acc_val_avg[-1] >= max_val_acc or data_cost_val_avg[-1] <= min_val_loss:\n",
    "            max_val_acc = np.maximum(acc_val_avg[-1], max_val_acc)\n",
    "            min_val_loss = np.minimum(data_cost_val_avg[-1], min_val_loss)\n",
    "            if acc_val_avg[-1] >= max_val_acc and data_cost_val_avg[-1] <= min_val_loss:\n",
    "                best_model_test_acc = acc_test_avg[-1]\n",
    "            curr_step = 0\n",
    "        else:\n",
    "            curr_step += 1\n",
    "            if curr_step == patience:\n",
    "                tqdm.write('Early stop! Min loss: ', min_val_loss, ', Max accuracy: ', max_val_acc)\n",
    "                break\n",
    "\n",
    "    tic = time.time()\n",
    "    feed_dict = {GCNN.idx_nodes: train_idx, GCNN.keep_prob: 0.4}\n",
    "\n",
    "    _, current_training_loss, norm_grad, current_acc_training = GCNN.session.run([GCNN.opt_step, GCNN.loss, GCNN.norm_grad, GCNN.accuracy], feed_dict) \n",
    "\n",
    "    training_time = time.time() - tic   \n",
    "\n",
    "    cost_train_avg.append(current_training_loss)\n",
    "    grad_norm_train_avg.append(norm_grad)\n",
    "    acc_train_avg.append(current_acc_training)\n",
    "\n",
    "\n",
    "#Compute and print statistics of the last realized experiment\n",
    "list_all_acc.append(100*best_model_test_acc)\n",
    "list_all_cost_val_avg.append(cost_val_avg)\n",
    "list_all_data_cost_val_avg.append(data_cost_val_avg)\n",
    "list_all_acc_val_avg.append(acc_val_avg)\n",
    "list_all_cost_test_avg.append(cost_test_avg)\n",
    "list_all_acc_test_avg.append(acc_test_avg)\n",
    "\n",
    "print('Num done: %d' % num_done)\n",
    "print('Max accuracy on test set achieved: %f%%' % np.max(np.asarray(acc_test_avg)*100))\n",
    "print('Max suggested accuracy: %f%%' % (100*best_model_test_acc))#(np.asarray(acc_test_avg)[np.asarray(data_cost_val_avg)==np.min(data_cost_val_avg)]),))\n",
    "print('Current mean: %f%%' % np.mean(list_all_acc))\n",
    "print('Current std: %f' % np.std(list_all_acc))\n",
    "\n",
    "num_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.25999975204468\n",
      "0.5765412469708043\n"
     ]
    }
   ],
   "source": [
    "#Print average performance\n",
    "print(np.mean(list_all_acc))\n",
    "print(np.std(list_all_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
