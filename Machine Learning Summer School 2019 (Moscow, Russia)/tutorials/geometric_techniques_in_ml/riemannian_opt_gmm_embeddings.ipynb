{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "riemannian_opt_gmm_embeddings.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnIQw3vrPEhl",
        "colab_type": "text"
      },
      "source": [
        "This is a tutorial notebook on Riemannian optimization for machine learning, prepared for the Machine Learning Summer School 2019 (MLSS-2019, http://mlss2019.skoltech.ru) in Moscow, Russia, Skoltech (http://skoltech.ru).\n",
        "\n",
        "Copyright 2019 by Alexey Artemov and ADASE 3DDL Team. Special thanks to Alexey Zaytsev for a valuable contribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MD6cSJaPEhn",
        "colab_type": "text"
      },
      "source": [
        "## Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNw_x4zrPEhn",
        "colab_type": "text"
      },
      "source": [
        "1. [Generate a toy dataset](#Generate-a-toy-dataset).\n",
        "2. [Use Riemannian optimization to obtain GMM estimates](#Use-Riemannian-optimization-to-obtain-GMM-estimates).\n",
        "3. [GMM with real-world data using Riemannian optimization](#GMM-with-real-world-data-using-Riemannian-optimization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h69X9iE7PEho",
        "colab_type": "text"
      },
      "source": [
        "## Riemannian Optimisation with `pymanopt` for inference in Gaussian mixture models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvi8wp7kPEhp",
        "colab_type": "text"
      },
      "source": [
        "This notebook is the second in the series of two notebooks on Riemannian optimization and is based heavily on the [official mixture of Gaussian notebook](https://github.com/pymanopt/pymanopt/blob/master/examples/MoG.ipynb) from `pymanopt` docs. \n",
        "\n",
        "For the basic introduction, see the first part `riemannian_opt_for_ml.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUbxow6wPEhp",
        "colab_type": "text"
      },
      "source": [
        "Install the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RqFzcLpPEhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade git+https://github.com/mlss-skoltech/tutorials.git#subdirectory=geometric_techniques_in_ML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOGvTsCrPEhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pymanopt autograd\n",
        "!pip install scipy==1.2.1 -U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZDQc1LfQBRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pkg_resources\n",
        "\n",
        "DATA_PATH = pkg_resources.resource_filename('riemannianoptimization', 'data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgkSzxeVPEhw",
        "colab_type": "text"
      },
      "source": [
        "### Generate a toy dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9auRDUvPEhx",
        "colab_type": "text"
      },
      "source": [
        "The Mixture of Gaussians (MoG) model assumes that datapoints $\\mathbf{x}_i\\in\\mathbb{R}^d$ follow a distribution described by the following probability density function:\n",
        "$$\n",
        "p(\\mathbf{x}) = \\sum_{m=1}^M \\pi_m p_\\mathcal{N}(\\mathbf{x};\\mathbf{\\mu}_m,\\mathbf{\\Sigma}_m)\n",
        "$$ \n",
        "\n",
        "where $\\pi_m$ is the probability that the data point belongs to the $m^\\text{th}$ mixture component and $p_\\mathcal{N}(\\mathbf{x};\\mathbf{\\mu}_m,\\mathbf{\\Sigma}_m)$ is the probability density function of a [multivariate Gaussian distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) with mean $\\mathbf{\\mu}_m \\in \\mathbb{R}^d$ and [positive semi-definite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix) (PSD) covariance matrix $\\mathbf{\\Sigma}_m \\in \\{\\mathbf{M}\\in\\mathbb{R}^{d\\times d}: \\mathbf{M}\\succeq 0\\}$.\n",
        "\n",
        "As an example consider the mixture of three Gaussians with means\n",
        "$$\n",
        "\\mathbf{\\mu}_1 = \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf{\\mu}_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf{\\mu}_3 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix},\n",
        "$$\n",
        "covariances\n",
        "$$\\mathbf{\\Sigma}_1 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix},\n",
        "\\mathbf{\\Sigma}_2 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 3 \\end{bmatrix},\n",
        "\\mathbf{\\Sigma}_3 = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$$\n",
        "and mixture probability vector $\\pi=\\left[0.1, 0.6, 0.3\\right]$.\n",
        "Let's generate $N=1000$ samples of that MoG model and scatter plot the samples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuYDX2UKPEhx",
        "colab_type": "text"
      },
      "source": [
        "Generate a synthetic dataset of $M=3$ Gaussian distributions, w"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ambBqXq6PEhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "toy_n_points = 1000  # Number of data\n",
        "toy_dim = 2  # Dimension of data\n",
        "toy_components = 3  # Number of clusters \n",
        "\n",
        "# mixture parameters\n",
        "toy_pi = [0.1, 0.6, 0.3]\n",
        "toy_mus = [np.array([-4, 1]),\n",
        "           np.array([0, 0]),\n",
        "           np.array([2, -1])]\n",
        "toy_sigmas = [np.array([[3, 0],[0, 1]]),\n",
        "              np.array([[1, 1.], [1, 3]]),\n",
        "              .5 * np.eye(2)]\n",
        "\n",
        "# select which component work in each case\n",
        "components = np.random.choice(toy_components, size=toy_n_points, p=toy_pi)\n",
        "\n",
        "# prepare data\n",
        "samples = np.zeros((toy_n_points, toy_dim))\n",
        "\n",
        "# for each component, generate all needed samples\n",
        "for k in range(toy_components):\n",
        "    # indices of current component in X\n",
        "    indices = (k == components)\n",
        "    # number of those occurrences\n",
        "    n_k = indices.sum()\n",
        "    if n_k > 0:\n",
        "        samples[indices] = np.random.multivariate_normal(toy_mus[k], toy_sigmas[k], n_k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrePOBAiPEh1",
        "colab_type": "text"
      },
      "source": [
        "The following is a bunch of helper functions for visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEM2wHcMPEh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm # Colormaps\n",
        "\n",
        "\n",
        "def multivariate_normal(x, d, mean, covariance):\n",
        "    \"\"\"pdf of the multivariate normal distribution.\"\"\"\n",
        "    x_m = x - mean\n",
        "    pdf = (1. / (np.sqrt((2 * np.pi)**d * np.linalg.det(covariance))) * \n",
        "            np.exp(-(np.linalg.solve(covariance, x_m).T.dot(x_m)) / 2))\n",
        "    return pdf\n",
        "\n",
        "\n",
        "# Plot bivariate distribution\n",
        "def generate_surface(mean, covariance, d):\n",
        "    \"\"\"Helper function to generate density surface.\"\"\"\n",
        "    nb_of_x = 100 # grid size\n",
        "    # choose limits adaptively\n",
        "#     mu1, mu2 = mean[:, 0]\n",
        "#     sigmasq1, sigmasq2 = covariance[0, 0], covariance[1, 1]\n",
        "#     min_x1 = mu1 - 3. * np.sqrt(sigmasq1)\n",
        "#     max_x1 = mu1 + 3. * np.sqrt(sigmasq1)\n",
        "#     min_x2 = mu2 - 3. * np.sqrt(sigmasq2)\n",
        "#     max_x2 = mu2 + 3. * np.sqrt(sigmasq2)\n",
        "#     print(min_x1, max_x1)\n",
        "#     print(min_x2, max_x2)\n",
        "    min_x1, max_x1 = -4, 4\n",
        "    min_x2, max_x2 = -4, 4\n",
        "    x1s = np.linspace(min_x1, max_x1, num=nb_of_x)\n",
        "    x2s = np.linspace(min_x2, max_x2, num=nb_of_x)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s) # Generate grid\n",
        "    pdf = np.zeros((nb_of_x, nb_of_x))\n",
        "    \n",
        "    # Fill the cost matrix for each combination of weights\n",
        "    for i in range(nb_of_x):\n",
        "        for j in range(nb_of_x):\n",
        "            pdf[i,j] = multivariate_normal(\n",
        "                np.matrix([[x1[i,j]], [x2[i,j]]]), \n",
        "                d, mean, covariance)\n",
        "    return x1, x2, pdf  # x1, x2, pdf(x1,x2)\n",
        "\n",
        "\n",
        "def plot_gaussian(mu, sigma, ax):\n",
        "    bivariate_mean = np.matrix(mu)  # Mean\n",
        "    bivariate_covariance = np.matrix(sigma)  # Covariance\n",
        "    x1, x2, p = generate_surface(\n",
        "        bivariate_mean, bivariate_covariance, d=2)    \n",
        "    # Plot bivariate distribution\n",
        "    con = ax.contour(x1, x2, p, 10, cmap=cm.hot)\n",
        "    # ax2.axis([-2.5, 2.5, -1.5, 3.5])\n",
        "    ax.set_aspect('equal')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIPDqDjLPEh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.gca()\n",
        "\n",
        "for mu, sigma in zip(toy_mus, toy_sigmas):\n",
        "    mu = np.matrix(mu).T  # plot_gaussian requires mu to be a column vector\n",
        "    plot_gaussian(mu, sigma, ax)\n",
        "\n",
        "colors = ['r', 'g', 'b', 'c', 'm']\n",
        "for i in range(toy_components):\n",
        "    indices = (i == components)\n",
        "    ax.scatter(samples[indices, 0], samples[indices, 1], alpha=.4, color=colors[i % toy_components])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zblgje-JPEh9",
        "colab_type": "text"
      },
      "source": [
        "### Use Riemannian optimization to obtain GMM estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l5-6FsOPEh-",
        "colab_type": "text"
      },
      "source": [
        "Given a data sample the de facto standard method to infer the parameters is the [expectation maximisation](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm) (EM) algorithm that, in alternating so-called E and M steps, maximises the log-likelihood of the data.\n",
        "\n",
        "In [arXiv:1506.07677](http://arxiv.org/pdf/1506.07677v1.pdf) Hosseini and Sra propose Riemannian optimisation as a powerful counterpart to EM. Importantly, they introduce a reparameterisation that leaves local optima of the log-likelihood unchanged while resulting in a geodesically convex optimisation problem over a product manifold $\\prod_{m=1}^M\\mathcal{PD}^{(d+1)\\times(d+1)}$ of manifolds of $(d+1)\\times(d+1)$ positive definite matrices.\n",
        "The proposed method is on par with EM and shows less variability in running times.\n",
        "\n",
        "The reparameterised optimisation problem for augmented data points $\\mathbf{y}_i=[\\mathbf{x}_i\\ 1]$ can be stated as follows:\n",
        "\n",
        "$$\\min_{(S_1, ..., S_m, \\nu_1, ..., \\nu_{m-1}) \\in \\prod_{m=1}^M \\mathcal{PD}^{(d+1)\\times(d+1)}\\times\\mathbb{R}^{M-1}}\n",
        "-\\sum_{n=1}^N\\log\\left(\n",
        "\\sum_{m=1}^M \\frac{\\exp(\\nu_m)}{\\sum_{k=1}^M\\exp(\\nu_k)}\n",
        "q_\\mathcal{N}(\\mathbf{y}_n;\\mathbf{S}_m)\n",
        "\\right)$$\n",
        "\n",
        "where\n",
        "\n",
        "* $\\mathcal{PD}^{(d+1)\\times(d+1)}$ is the manifold of positive definite\n",
        "$(d+1)\\times(d+1)$ matrices\n",
        "* $\\mathcal{\\nu}_m = \\log\\left(\\frac{\\alpha_m}{\\alpha_M}\\right), \\ m=1, ..., M-1$ and $\\nu_M=0$\n",
        "* $q_\\mathcal{N}(\\mathbf{y}_n;\\mathbf{S}_m) =\n",
        "2\\pi\\exp\\left(\\frac{1}{2}\\right)\n",
        "|\\operatorname{det}(\\mathbf{S}_m)|^{-\\frac{1}{2}}(2\\pi)^{-\\frac{d+1}{2}}\n",
        "\\exp\\left(-\\frac{1}{2}\\mathbf{y}_i^\\top\\mathbf{S}_m^{-1}\\mathbf{y}_i\\right)$\n",
        "\n",
        "**Optimisation problems like this can easily be solved using Pymanopt – even without the need to differentiate the cost function manually!**\n",
        "\n",
        "So let's infer the parameters of our toy example by Riemannian optimisation using Pymanopt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYOYVfMPEh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymanopt as opt\n",
        "import pymanopt.solvers as solvers\n",
        "import pymanopt.manifolds as manifolds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYYpl8X4PEiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np  # import here to avoid errors\n",
        "from autograd.scipy.misc import logsumexp\n",
        "\n",
        "# (1) Instantiate the manifold\n",
        "manifold = manifolds.Product([\n",
        "    manifolds.PositiveDefinite(toy_dim + 1, k=toy_components), \n",
        "    manifolds.Euclidean(toy_components - 1)\n",
        "])\n",
        "\n",
        "# (2) Define cost function\n",
        "# The parameters must be contained in a list theta.\n",
        "def cost(theta):\n",
        "    # Unpack parameters\n",
        "    nu = np.concatenate([theta[1], [0]], axis=0)\n",
        "    \n",
        "    S = theta[0]\n",
        "    logdetS = np.expand_dims(np.linalg.slogdet(S)[1], 1)\n",
        "    y = np.concatenate([samples.T, np.ones((1, len(samples)))], axis=0)\n",
        "\n",
        "    # Calculate log_q\n",
        "    y = np.expand_dims(y, 0)\n",
        "    \n",
        "    # 'Probability' of y belonging to each cluster\n",
        "    log_q = -0.5 * (np.sum(y * np.linalg.solve(S, y), axis=1) + logdetS)\n",
        "\n",
        "    alpha = np.exp(nu)\n",
        "    alpha = alpha / np.sum(alpha)\n",
        "    alpha = np.expand_dims(alpha, 1)\n",
        "    \n",
        "    loglikvec = logsumexp(np.log(alpha) + log_q, axis=0)\n",
        "    return -np.sum(loglikvec)\n",
        "\n",
        "\n",
        "problem = opt.Problem(manifold=manifold, cost=cost, verbosity=2)\n",
        "\n",
        "# (3) Instantiate a Pymanopt solver\n",
        "solver = solvers.SteepestDescent()\n",
        "\n",
        "# let Pymanopt do the rest\n",
        "Xopt = solver.solve(problem)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yvE_pHbPEiE",
        "colab_type": "text"
      },
      "source": [
        "Once Pymanopt has finished the optimisation we can obtain the inferred parameters as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIxF4cPzPEiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_gaussian_parameters(Xopt, n=1):\n",
        "    params, probas = Xopt\n",
        "    \n",
        "    mus, sigmas = [], []\n",
        "    \n",
        "    for p in params:\n",
        "        mu = p[0:2,2:3]\n",
        "        sigma = p[:2, :2] - mu.dot(mu.T)\n",
        "        mus.append(mu)\n",
        "        sigmas.append(sigma)\n",
        "    \n",
        "    pis = np.exp(np.concatenate([probas, [0]], axis=0))\n",
        "    pis = pis / np.sum(pis)\n",
        "    \n",
        "    return mus, sigmas, pis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7H7_-WMPEiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_mus_opt, toy_sigmas_opt, toy_pis_opt = extract_gaussian_parameters(Xopt, n=3)\n",
        "toy_mus_opt, toy_sigmas_opt, toy_pis_opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21rmQP1iPEiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.gca()\n",
        "\n",
        "for mu, sigma in zip(toy_mus_opt, toy_sigmas_opt):\n",
        "    plot_gaussian(mu, sigma, ax)\n",
        "\n",
        "colors = ['r', 'g', 'b', 'c', 'm']\n",
        "for i in range(toy_components):\n",
        "    indices = (i == components)\n",
        "    ax.scatter(samples[indices, 0], samples[indices, 1], alpha=.4, color=colors[i % toy_components])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMPEQeFgPEiL",
        "colab_type": "text"
      },
      "source": [
        "And convince ourselves that the inferred parameters are close to the ground truth parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdO9uKI3PEiM",
        "colab_type": "text"
      },
      "source": [
        "### GMM with real-world data using Riemannian optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o9UHzqXPEiN",
        "colab_type": "text"
      },
      "source": [
        "Certain real-world datasets can be sufficiently closely modelled by the GMM. One instance might be low-dimensional word embeddings. An accompanying notebook `riemannian_opt_text_preprocessing.ipynb` details how these data were obtained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9g3ModaPEiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(DATA_PATH + 'tsne_result_training_part.csv', index_col=0)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD4oYVAkPEiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = df[['x', 'y']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amEL6JivPEiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIiQiVMBPEiT",
        "colab_type": "text"
      },
      "source": [
        "For the optimization to be a little more stable, we standardize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EFQFPxXPEiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "samples = StandardScaler().fit_transform(samples)\n",
        "\n",
        "plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-7O_l5mPEiW",
        "colab_type": "text"
      },
      "source": [
        "Use pretty much the same codes as above, changing the number of components and sample size accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s49QjNaPPEiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_components = 4\n",
        "real_dim = 2\n",
        "real_points = len(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AItZaTnPEie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np  # import here to avoid errors\n",
        "from autograd.scipy.misc import logsumexp\n",
        "\n",
        "# (1) Instantiate the manifold\n",
        "manifold = manifolds.Product([\n",
        "    manifolds.PositiveDefinite(real_dim + 1, k=real_components), \n",
        "    manifolds.Euclidean(real_components - 1)\n",
        "])\n",
        "\n",
        "# (2) Define cost function\n",
        "# The parameters must be contained in a list theta.\n",
        "def cost(theta):\n",
        "    # Unpack parameters\n",
        "    nu = np.concatenate([theta[1], [0]], axis=0)\n",
        "    \n",
        "    S = theta[0]\n",
        "    logdetS = np.expand_dims(np.linalg.slogdet(S)[1], 1)\n",
        "    y = np.concatenate([samples.T, np.ones((1, real_points))], axis=0)\n",
        "\n",
        "    # Calculate log_q\n",
        "    y = np.expand_dims(y, 0)\n",
        "    \n",
        "    # 'Probability' of y belonging to each cluster\n",
        "    log_q = -0.5 * (np.sum(y * np.linalg.solve(S, y), axis=1) + logdetS)\n",
        "\n",
        "    alpha = np.exp(nu)\n",
        "    alpha = alpha / np.sum(alpha)\n",
        "    alpha = np.expand_dims(alpha, 1)\n",
        "    \n",
        "    loglikvec = logsumexp(np.log(alpha) + log_q, axis=0)\n",
        "    return -np.sum(loglikvec)\n",
        "\n",
        "\n",
        "problem = opt.Problem(manifold=manifold, cost=cost, verbosity=2)\n",
        "\n",
        "# (3) Instantiate a Pymanopt solver\n",
        "solver = solvers.SteepestDescent()\n",
        "\n",
        "# let Pymanopt do the rest\n",
        "Xopt = solver.solve(problem)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwiwvxgSPEih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_mus_opt, real_sigmas_opt, real_pis_opt = extract_gaussian_parameters(Xopt, n=3)\n",
        "real_mus_opt, real_sigmas_opt, real_pis_opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sJjPaye6PEij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.gca()\n",
        "\n",
        "for mu, sigma in zip(real_mus_opt, real_sigmas_opt):\n",
        "    plot_gaussian(mu, sigma, ax)\n",
        "\n",
        "ax.scatter(samples[:, 0], samples[:, 1], alpha=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLge4dRLPEil",
        "colab_type": "text"
      },
      "source": [
        "Et voilà – this was a brief demonstration of how to do inference for MoG models by performing Manifold optimisation using Pymanopt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPVLD88NPEil",
        "colab_type": "text"
      },
      "source": [
        "**TODO HOMEWORK** add riemannian optimization in M-step to speed up EM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63OLLMvtPEim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}