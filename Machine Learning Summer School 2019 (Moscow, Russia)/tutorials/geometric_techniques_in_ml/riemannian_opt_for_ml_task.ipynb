{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "riemannian_opt_for_ml.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinaMaz/mlss-tutorials/blob/master/solomon-embeddings-tutorial/riemannian_opt_for_ml_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "brtqoI8L61Cs"
      },
      "source": [
        "This is a tutorial notebook on Riemannian optimization for machine learning, prepared for the Machine Learning Summer School 2019 (MLSS-2019, http://mlss2019.skoltech.ru) in Moscow, Russia, Skoltech (http://skoltech.ru).\n",
        "\n",
        "Copyright 2019 by Alexey Artemov and ADASE 3DDL Team. Special thanks to Alexey Zaytsev for a valuable contribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE6KSsUa61Cu"
      },
      "source": [
        "## Riemannian optimization for machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p2naRIBA61Cu"
      },
      "source": [
        "The purpose of this tutorial is to give a gentle introduction into the practice of Riemannian optimization. You will learn to: \n",
        "\n",
        " 1. Reformulate familiar optimization problems in terms of Riemannian optimization on manifolds.\n",
        " 2. Use a Riemannian optimization library `pymanopt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b9iBeGGM61Cv"
      },
      "source": [
        "## Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_JrgWjhy61Cw"
      },
      "source": [
        "1. [Recap and the introduction: linear regression](#Recap-and-the-introduction:-linear-regression).\n",
        "2. [Introduction into ManOpt and pymanopt](#Intoduction-into-ManOpt-package-for-Riemannian-optimization).\n",
        "3. [Learning the shape space of facial landmarks](#Learning-the-shape-space-of-facial-landmarks): \n",
        " - [Problem formulation and general reference](#Problem-formulation-and-general-reference).\n",
        " - [Procrustes analysis for the alignment of facial landmarks](#Procrustes-analysis-for-the-alignment-of-facial-landmarks).\n",
        " - [PCA for learning the shape space](#PCA-for-learning-the-shape-space).\n",
        "4. [Analysing the shape space of facial landmarks via MDS](#Analysing-the-shape-space-of-facial-landmarks-via-MDS).\n",
        "5. [Learning the Gaussian mixture models for word embeddings](#Learning-the-Gaussian-mixture-models-for-word-embeddings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WPY9Wc2V61Cw"
      },
      "source": [
        "Install the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xH_5Jt3261Cx",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade git+https://github.com/mlss-skoltech/tutorials.git#subdirectory=geometric_techniques_in_ML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3VmJCBVj61Cz",
        "colab": {}
      },
      "source": [
        "!pip install pymanopt autograd\n",
        "!pip install scipy==1.2.1 -U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z8U9Rxs_81tK",
        "colab": {}
      },
      "source": [
        "import pkg_resources\n",
        "\n",
        "DATA_PATH = pkg_resources.resource_filename('riemannianoptimization', 'data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p12dofCN61C2"
      },
      "source": [
        "## Recap and the introduction: linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aYxJ6l_u61C2"
      },
      "source": [
        "_NB: This section of the notebook is for illustrative purposes only, no code input required_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qKE7R_4v61C3"
      },
      "source": [
        "#### Recall the maths behind it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R2sH1xnw61C3"
      },
      "source": [
        "We're commonly working with a problem of finding the weights $w \\in \\mathbb{R}^n$ such that\n",
        "$$\n",
        "||\\mathbf{y} - \\mathbf{X} \\mathbf{w}||^2_2 \\to \\min_{\\mathbf{w}},\n",
        "$$\n",
        "with $\\mathbf{x}_i \\in \\mathbb{R}^n$, i.e. features are vectors of numbers, and $y_i \\in \\mathbb{R}$.\n",
        "$\\mathbf{X} \\in \\mathbb{R}^{\\ell \\times n}$ is a matrix with $\\ell$ objects and $n$ features.\n",
        "\n",
        "A commonly computed least squares solution is of the form: \n",
        "$$\n",
        "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\n",
        "$$\n",
        "\n",
        "We could account for the non-zero mean case ($\\mathrm{E} \\mathbf{y} \\neq 0$) by either adding and subtracting the mean, or by using an additional feature in $\\mathbf{X}$ set to all ones.\n",
        "\n",
        "The solution could simply be computed via:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CnyV3vjW61C4",
        "colab": {}
      },
      "source": [
        "def compute_weights_multivariate(X, y):\n",
        "    \"\"\"\n",
        "    Given feature array X [n_samples, 1], target vector y [n_samples],\n",
        "    compute the optimal least squares solution using the formulae above.\n",
        "    For brevity, no bias term!\n",
        "    \"\"\"\n",
        "    # Compute the \"inverting operator\"\n",
        "    R = np.dot(\n",
        "        np.linalg.inv(\n",
        "            np.dot(X.T, X)\n",
        "        ), X.T\n",
        "    )\n",
        "    # Compute the actual solution\n",
        "    w = np.dot(R, y)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-UJ7RVsB61C5"
      },
      "source": [
        "#### Recall the gradient descent solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zY_j15oF61C6"
      },
      "source": [
        "Let us view\n",
        "$$\n",
        "L(\\mathbf{y}, \\mathbf{X} \\mathbf{w}) = \\frac{1}{\\ell} ||\\mathbf{y} - \\mathbf{X} \\mathbf{w}||^2_2 \n",
        "     \\to \\min_{\\mathbf{w}},\n",
        "$$\n",
        "as pure unconstrained optimization problem of the type \n",
        "$$\n",
        "f(\\mathbf{w}) \\to \\min\\limits_{\\mathbf{w} \\in \\mathbb{R}^n}\n",
        "$$\n",
        "with $f(\\mathbf{w}) \\equiv L(\\mathbf{y}, \\mathbf{X} \\mathbf{w})$.\n",
        "\n",
        "To use the gradient descent, we must \n",
        "* initialize the weights $\\mathbf{w}$ somehow,\n",
        "* find a way of computing the __gradient__ of our quality measure $L(\\mathbf{y}, \\widehat{\\mathbf{y}})$ w.r.t. $\\mathbf{w}$,\n",
        "* starting from the initialization, iteratively update weights using the gradient descent: \n",
        "$$\n",
        "\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\gamma \\nabla_{\\mathbf{w}} L,\n",
        "$$\n",
        "where $\\gamma$ is step size.\n",
        "\n",
        "Since we choose $L(\\mathbf{y}, \\widehat{\\mathbf{y}}) \\equiv \\frac 1 \\ell ||\\mathbf{y} - \\mathbf{X} \\mathbf{w} ||^2$, our gradient is $ \\frac 2 \\ell \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{w})  $.\n",
        "\n",
        "The solution is coded by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "COnR-1xc61C7",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def compute_gradient(X, y, w):\n",
        "    \"\"\"\n",
        "    Computes the gradient of MSE loss \n",
        "    for multivariate linear regression of X onto y \n",
        "    w.r.t. w, evaluated at the current w.\n",
        "    \"\"\"\n",
        "    prediction = np.dot(X, w)  # [n_objects, n_features] * [n_features] -> [n_objects]\n",
        "    error = prediction - y  # [n_objects]\n",
        "    return 2 * np.dot(error, X) / len(error)  # [n_objects] * [n_objects, n_features] -> [n_features]\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, w_init, iterations=1, gamma=0.01):\n",
        "    \"\"\"\n",
        "    Performs the required number of iterations of gradient descent.\n",
        "    Parameters:\n",
        "        X [n_objects, n_features]: matrix of featues\n",
        "        y [n_objects]: responce (dependent) variable\n",
        "        w_init: the value of w used as an initializer\n",
        "        iterations: number of steps for gradient descent to compute\n",
        "        gamma: learning rate (gradient multiplier)\n",
        "    \"\"\"\n",
        "    costs, grads, ws = [], [], []\n",
        "    w = w_init\n",
        "    for i in range(iterations):\n",
        "        # Compute our cost in current point (before the gradient step)\n",
        "        costs.append(mean_squared_error(y, np.dot(X, w)) / len(y))\n",
        "        # Remember our weights w in current point\n",
        "        ws.append(w)\n",
        "        # Compute gradient for w\n",
        "        w_grad = compute_gradient(X, y, w)\n",
        "        grads.append(w_grad)\n",
        "        # Update the current weight w using the formula above (see comments)\n",
        "        w = w - gamma * w_grad\n",
        "    # record the last weight\n",
        "    ws.append(w)\n",
        "    return costs, grads, ws"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vt_Ydy6S61C8"
      },
      "source": [
        "## Intoduction into ManOpt package for Riemannian optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MKAYzB5h61C9"
      },
      "source": [
        "#### `ManOpt` and `pymanopt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C55s4PTk61C9"
      },
      "source": [
        "The Matlab library `ManOpt` (https://www.manopt.org) and its Python version `pymanopt` (http://pymanopt.github.io) are versatile toolboxes for optimization on manifolds. \n",
        "\n",
        "The two libraries are built so that they separate the _manifolds_, the _solvers_ and the _problem descriptions_. For basic use, one only needs to:\n",
        " * pick a manifold from the library, \n",
        " * describe the cost function (and possible derivatives) on this manifold, and \n",
        " * pass it on to a solver. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QRp90upm61C-"
      },
      "source": [
        "_NB: The purpose of the following is to get familiar with pymanopt and to serve as a reference point when coding your own optimization problems._\n",
        "\n",
        "To start working with `pymanopt`, you'll need the following \n",
        "\n",
        " 1. Import the necessary backend for automatic differentiation\n",
        "\n",
        "```python\n",
        "import autograd.numpy as np```\n",
        "but theano and TensorFlow backends are supported, too. \n",
        "\n",
        "We will also require importing `pymanopt` itself, along with the necessary submodules:\n",
        "```python\n",
        "import pymanopt as opt\n",
        "import pymanopt.solvers as solvers\n",
        "import pymanopt.manifolds as manifolds```\n",
        "\n",
        " 2. Define (or rather, select) the manifold of interest. `pymanopt` provides a [large number](https://pymanopt.github.io/doc/#manifolds) of predefined manifold classes (however, a lot less than the [original ManOpt Matlab library](https://www.manopt.org/tutorial.html#manifolds)). E.g., to instantiate a manifold $V_{2}(\\mathbb {R}^{5}) = \\{X \\in \\mathbb{R}^{5 \\times 2} : X^TX = I_2\\}^k$ of orthogonal projection matrices from $\\mathbb{R}^5$ to $\\mathbb{R}^2$ you will write:\n",
        "\n",
        "```python\n",
        "manifold = manifolds.Stiefel(5, 2)```\n",
        "\n",
        "Available manifolds include [Steifel](https://pymanopt.github.io/doc/#module-pymanopt.manifolds.stiefel) ([wiki](https://en.wikipedia.org/wiki/Stiefel_manifold)), Rotations or SO(n) ([wiki](https://en.wikipedia.org/wiki/Orthogonal_group)), [Euclidean](https://pymanopt.github.io/doc/#module-pymanopt.manifolds.euclidean), [Positive Definite](https://pymanopt.github.io/doc/#pymanopt.manifolds.psd.PositiveDefinite) ([wiki](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix)), and [Product](https://pymanopt.github.io/doc/#pymanopt.manifolds.product.Product), along many others.\n",
        "\n",
        " 3. Define the **scalar** cost function (here using `autograd.numpy`) to be minimized by the \n",
        "```python\n",
        "def cost(X):  return np.sum(X)```\n",
        "\n",
        "Note that the scalar `cost` python function **will have access to objects defined elsewhere in code** (which allows accessing $X$ and $y$ for optimization).\n",
        "\n",
        " 4. Instantiate the `pymanopt` problem\n",
        "```python\n",
        "problem = opt.Problem(manifold=manifold, cost=cost, verbosity=2)```\n",
        "The keyword `verbosity` controls hwo much output you get from the system (smaller values mean less output).\n",
        "\n",
        " 5. Instantiate a `pymanopt` solver, e.g.:\n",
        "```python\n",
        "solver = solvers.SteepestDescent()```\n",
        "The library has a lot of solvers implemented, including SteepestDescent, TrustRegions, ConjugateGradient, and NelderMead objects.\n",
        "\n",
        " 6. Perform the optimization in a single blocking function call, obtaining the optimal value of the desired quantity:\n",
        "```python\n",
        "Xopt = solver.solve(problem)```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NG5zUPxx61C_"
      },
      "source": [
        "#### Linear regression using `pymanopt`\n",
        "_The purpose of this section is to get the first hands-out experience using `pymanopt`. We compare its output with hand-coded gradient descent and the analytic solution._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YhgD1X1J61DA",
        "colab": {}
      },
      "source": [
        "import pymanopt as opt\n",
        "import pymanopt.solvers as solvers\n",
        "import pymanopt.manifolds as manifolds\n",
        "\n",
        "# Import the differentiable numpy -- this is crucial, \n",
        "# as `np` conventionally imported will not provide gradients.\n",
        "# See more at https://github.com/HIPS/autograd\n",
        "import autograd.numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHd5Ht5O61DE",
        "colab": {}
      },
      "source": [
        "# Generate random data\n",
        "X = np.random.randn(200, 3)\n",
        "y = np.random.randint(-5, 5, (200))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AtKsDA-B61DF"
      },
      "source": [
        "**Exercise:** program the linear regression using manifold optimization\n",
        "\n",
        "**Hint:** create `Euclidean` manifold and the `SteepestDescent` solver. \n",
        "\n",
        "**Hint:** write down the formula for the cost. Remember it has the access to `X` and `y` defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k9EKzLdM61DG",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np  # import again to avoid errors \n",
        "\n",
        "# Cost function is the squared error. Remember, cost is a scalar value!\n",
        "def cost(w):\n",
        "     return # <your code here>\n",
        "\n",
        "# A simplest possible solver (gradient descent)\n",
        "solver = # <your code here>\n",
        "\n",
        "# R^3\n",
        "manifold = # <your code here>\n",
        "\n",
        "# Solve the problem with pymanopt\n",
        "problem = opt.Problem(manifold=manifold, cost=cost)\n",
        "wopt = solver.solve(problem)\n",
        "\n",
        "print('The following regression weights were found to minimise the '\n",
        "      'squared error:')\n",
        "print(wopt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kQwuHGqx61DI"
      },
      "source": [
        "Compute the linear regression solution via numerical optimization using steepest descent over the Euclidean manifold $\\mathbb{R}^3$, _only using our handcrafted gradient descent_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cNMn20Nl61DJ",
        "colab": {}
      },
      "source": [
        "gd_params = dict(w_init=np.random.rand(X.shape[1]),\n",
        "                 iterations=20,\n",
        "                 gamma=0.1)\n",
        "costs, grads, ws = gradient_descent(X, y, **gd_params)\n",
        "print(\" iter\\t\\t   cost val\\t    grad. norm\")\n",
        "for iteration, (cost, grad, w) in enumerate(zip(costs, grads, ws)):\n",
        "    gradnorm = np.linalg.norm(grad)\n",
        "    print(\"%5d\\t%+.16e\\t%.8e\" % (iteration, cost, gradnorm))\n",
        "\n",
        "print('\\nThe following regression weights were found to minimise the '\n",
        "      'squared error:')\n",
        "print(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UENl0Sje61DK"
      },
      "source": [
        "Finally, use the analytic formula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AFWAMA7V61DL",
        "colab": {}
      },
      "source": [
        "print('The closed form solution to this regression problem is:')\n",
        "\n",
        "compute_weights_multivariate(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SPM2kxLB61DN"
      },
      "source": [
        "Recall that you can always look what's inside by either reading the [developer docs](https://pymanopt.github.io/doc/) or simply examining the code via typing:\n",
        "```python\n",
        "solvers.SteepestDescent??```\n",
        "\n",
        "Compare the code there with our hand-crafted gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JmrEVRLE61DN"
      },
      "source": [
        "## Learning the shape space of facial landmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2HroJkEQ61DN"
      },
      "source": [
        "#### Problem formulation and general reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yprCp_N361DO"
      },
      "source": [
        "In this part, we will create the shape space of facial landmarks. Building such a shape space is of great interest in computer vision area, where numerous applications such as face detection, facial pose regression, and emotion recognition depend heavily on such models. Here are the basics of what one needs to know to proceed with this tutorial.\n",
        "\n",
        "1. [Active Shape Models](https://en.wikipedia.org/wiki/Active_shape_model) are a class of statistical shape models that can iteratively deform to fit to an example of the object in a image. They are commonly build by analyzing variations in points distributions and _encode plausible variations, allowing one to discriminate them from unlikely ones_.\n",
        "2. One great reference for all ASMs is Tim Cootes' paper: _Cootes, T., Baldock, E. R., & Graham, J. (2000)._ [An introduction to active shape models](https://person.hst.aau.dk/lasse/teaching/IACV/doc/asm_overview.pdf). _Image processing and analysis, 223-248._ It includes motivation, math, and algorithms behind the ASM.\n",
        "3. Nice reference implementations of the Active Shape Model for faces include, e.g., [this Matlab code](https://github.com/johnwmillr/ActiveShapeModels) and [this one, featuring additionally dental image analysis](https://github.com/LennartCockx/Python-Active-shape-model-for-Incisor-Segmentation).\n",
        "4. Production libraries such as [dlib](http://dlib.net) implement their own ASMs of facial landmarks.\n",
        "\n",
        "![Example of facial landmarks](https://neerajkumar.org/databases/lfpw/index_files/image002.png) (image taken from [Neeraj Kumar's page on LPFW](https://neerajkumar.org/databases/lfpw/))\n",
        "\n",
        "We will (1) [look at the data](#Obtain-and-view-the-dataset),\n",
        "(2) [align shapes](#Procrustes-analysis-for-the-alignment-of-facial-landmarks),\n",
        "and (3) [compute the shape space](#PCA-for-learning-the-shape-space)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SXO4Mgal61DO"
      },
      "source": [
        "### Obtain and view the dataset\n",
        "_The goal of this section is to examine the dataset._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wxjsEk2961DP",
        "colab": {}
      },
      "source": [
        "from riemannianoptimization.tutorial_helpers import load_data, plot_landmarks\n",
        "landmarks = load_data(DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R6T0GSrY61DR"
      },
      "source": [
        "View a random subset of the data. Run the cell below multiple times to view different subsets.\n",
        "\n",
        "You can set `draw_landmark_id` and `draw_landmarks` to 0 to turn them off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rGtDJBU661DR",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = np.random.choice(len(landmarks), size=6) # sample random faces\n",
        "\n",
        "fig, axs = plt.subplots(ncols=6, nrows=1, figsize=(18, 3))\n",
        "for ax, image in zip(axs, landmarks[idx]):\n",
        "    plot_landmarks(image, ax=ax, draw_landmark_id=1, draw_landmarks=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ca_RaVVI61DT"
      },
      "source": [
        "### Procrustes analysis for the alignment of facial landmarks\n",
        "_The purpose of this section is to learn how to use manifold optimization for shape alignment_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hQLdDyxT61DT"
      },
      "source": [
        "One thing to note is that the landmarks are annotated in images with different resolution and are generally **misaligned**. One can easily understand this by observing landmark scatterplots. Subtracting the mean shape or standardizing the points doesn't help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2K5nl96Y61DU",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(figsize=(15, 5), ncols=3)\n",
        "ax1.scatter(landmarks[:, 0::2], -landmarks[:, 1::2], alpha=.01)\n",
        "\n",
        "# compute the mean shape \n",
        "mean_shape = np.mean(landmarks, axis=0)\n",
        "landmarks_centered = landmarks - mean_shape\n",
        "ax2.scatter(landmarks_centered[:, 0::2], -landmarks_centered[:, 1::2], alpha=.01)\n",
        "\n",
        "# compute additionally the standard deviation in shape\n",
        "std_shape = np.std(landmarks, axis=0)\n",
        "landmarks_standardized = landmarks_centered / std_shape\n",
        "ax3.scatter(landmarks_standardized[:, 0::2], -landmarks_standardized[:, 1::2], alpha=.01);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y54L4Fa961DW"
      },
      "source": [
        "**Q:** Why such variation? Why we don't see separate clusters of  \"average keypoints\", like average eye1, eye2, and etc.\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oruqnfg561DX"
      },
      "source": [
        "We must _align_ shapes to a _canonical pose_ to proceed with building the ASM.\n",
        "\n",
        "This will be done in a simple way via [Procrustes analysis](https://en.wikipedia.org/wiki/Procrustes_analysis). In its simplest form, Procrustes analysis aligns each shape so that the sum of distances of each shape to the mean $D = \\sum\\limits_i ||\\mathbf{x}_i − \\mathbf{\\overline{x}}||^2_2)$ is minimised:\n",
        "1. Translate each example so that its center of gravity is at the origin.\n",
        "2. Choose one example as an initial estimate of the mean shape and scale.\n",
        "3. Record the first estimate as $\\overline{x}_0$ to define the default orientation.\n",
        "4. Align all the shapes with the current estimate of the mean shape.\n",
        "5. Re-estimate the mean from aligned shapes.\n",
        "6. Apply constraints on scale and orientation to the current estimate of the mean by aligning it with x ̄0 and scaling so that $|\\overline{x}| = 1$.\n",
        "7. If not converged, return to 4.\n",
        "(Convergence is declared if the estimate of the mean does not change\n",
        "significantly after an iteration)\n",
        "\n",
        " \n",
        "![Procrustes](https://upload.wikimedia.org/wikipedia/commons/f/f5/Procrustes_superimposition.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wpObFqu861DY",
        "colab": {}
      },
      "source": [
        "# A small helper function we will need \n",
        "# to center the shape at the origin and scale it to a unit norm.\n",
        "def standardize(shape):\n",
        "    # shape must have the shape [n_landmarks, 2], e.g. [35, 2]\n",
        "    shape -= np.mean(shape, 0)\n",
        "    shape_norm = np.linalg.norm(shape)\n",
        "    shape /= shape_norm\n",
        "    return shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gv8ah3eE61Da",
        "colab": {}
      },
      "source": [
        "# A large helper function that we will employ to align\n",
        "# the *entire collection* of shapes -- skip for now.\n",
        "def align_landmarks(landmarks, mean_shape=None, aligner=None, n_iterations=1):\n",
        "    \"\"\"\n",
        "    Aligns landmarks to an estimated mean shape.\n",
        "    In this function, `landmarks` are always assumed to be array of shape [n, 35, 2].\n",
        "    \n",
        "    aligner: a function getting two arguments (mean_shape and shape), returning\n",
        "             the transformation from shape to mean_shape\n",
        "    \"\"\"\n",
        "\n",
        "    # Translate each example so that its center of gravity is at the origin.\n",
        "    landmarks -= np.mean(landmarks, axis=1, keepdims=True)\n",
        "    \n",
        "    # Choose one example as an initial estimate of the mean shape and scale \n",
        "    # so that |x ̄| = 􏰆x ̄21 + y ̄12 + x ̄2 . . . = 1.\n",
        "    mean_shape = np.mean(landmarks, axis=0)\n",
        "    mean_shape = standardize(mean_shape)\n",
        "\n",
        "    # Record the first estimate as x0 to define the default orientation.\n",
        "    mean_shape_0 = mean_shape[:]\n",
        "    \n",
        "    def align_to_mean(landmarks, mean_shape, aligner=None):        \n",
        "        aligned_landmarks = []\n",
        "        for shape in landmarks:\n",
        "            shape = standardize(shape)\n",
        "            shape = aligner(mean_shape, shape)\n",
        "            aligned_landmarks.append(shape)\n",
        "        return np.array(aligned_landmarks)\n",
        "\n",
        "    print(\" iter\\t     cost val.\\t    mean diff.\")\n",
        "    for iteration in range(n_iterations):\n",
        "        # Align all the shapes with the current estimate of the mean shape.\n",
        "        aligned_landmarks = align_to_mean(landmarks, mean_shape, aligner=aligner)\n",
        "\n",
        "        mean_shape_prev = mean_shape\n",
        "        # Re-estimate the mean from aligned shapes.\n",
        "        mean_shape = np.mean(aligned_landmarks, axis=0)\n",
        "    \n",
        "        # Apply constraints on scale and orientation to the current \n",
        "        # estimate of the mean by aligning it with x ̄0 and scaling so that |x ̄| = 1.\n",
        "        mean_shape = aligner(mean_shape_0, mean_shape)\n",
        "        mean_shape /= np.linalg.norm(mean_shape)\n",
        "        \n",
        "        cost = np.sum(\n",
        "            np.linalg.norm(aligned_landmarks - mean_shape, axis=(1, 2))\n",
        "        )\n",
        "        mean_shape_diff = np.linalg.norm(mean_shape - mean_shape_prev)\n",
        "        print(\"%5d\\t%+.8e\\t%.8e\" % (iteration, cost, mean_shape_diff))\n",
        "\n",
        "    # If not converged, return to 4. \n",
        "    # (Convergence is declared if the estimate of the mean does not change significantly after an iteration)\n",
        "    return np.array(aligned_landmarks), mean_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "biGhCJN061Dc",
        "colab": {}
      },
      "source": [
        "landmarks = landmarks.reshape(-1, 35, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0CQ32xpU61Dd"
      },
      "source": [
        "One may naturally resort to [scipy.spatial.procrustes](https://docs.scipy.org/doc/scipy-1.2.1/reference/generated/scipy.spatial.procrustes.html), which computes an optimal alignment using a scale vector $\\mathbf{s}$ and a rotation matrix $\\mathbf{R}$, solving [orthogonal Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U3seV96n61De"
      },
      "source": [
        "**Exercise:** Using `scipy.spatial.procrustes`, write a default aligner function for our `align_landmarks`. This function must accept two shapes and return the second one aligned to the first one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6s6nDna761Df",
        "colab": {}
      },
      "source": [
        "from scipy.spatial import procrustes\n",
        "\n",
        "def default_procrustes(target_shape, source_shape):\n",
        "    \"\"\"Align the source shape to the target shape.\n",
        "    For standardized shapes, can skip translating/scaling \n",
        "    aligned source by target's parameters.\n",
        "    \n",
        "    target_shape, source_shape: ndarrays of shape [35, 2]\n",
        "    \n",
        "    return ndarray of shape [35, 2]\n",
        "    \"\"\"\n",
        "    # <your code here>\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tYBGkyF361Dl",
        "colab": {}
      },
      "source": [
        "# Try aligning a single shape \n",
        "mean_shape = np.mean(landmarks, axis=0)\n",
        "mean_shape = standardize(mean_shape)\n",
        "\n",
        "shape_std = standardize(landmarks[400])\n",
        "\n",
        "aligned_shape = default_procrustes(mean_shape, shape_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xhN9XrW661Dn",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(figsize=(15, 5), ncols=3)\n",
        "plot_landmarks(mean_shape, ax=ax1)\n",
        "ax1.set_title('Mean shape')\n",
        "\n",
        "# compute the mean shape \n",
        "plot_landmarks(mean_shape, ax=ax2, color_landmarks='grey', color_contour='grey', alpha=0.5)\n",
        "plot_landmarks(shape_std, ax=ax2)\n",
        "ax2.set_title('Another shape, distance = {0:.3f}'.format(np.linalg.norm(mean_shape - shape_std)))\n",
        "\n",
        "# compute additionally the standard deviation in shape\n",
        "plot_landmarks(mean_shape, ax=ax3, color_landmarks='grey', color_contour='grey', alpha=0.5)\n",
        "plot_landmarks(aligned_shape, ax=ax3)\n",
        "ax3.set_title('Aligned shapes, distance = {0:.3f}'.format(np.linalg.norm(mean_shape - aligned_shape)));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T6WreJWo61Do",
        "colab": {}
      },
      "source": [
        "# Align the entire dataset to a mean shape\n",
        "aligned_landmarks, mean_shape = align_landmarks(landmarks, aligner=default_procrustes, n_iterations=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgD2DsZS61Dq",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n",
        "ax1.scatter(aligned_landmarks[:, :, 0], -aligned_landmarks[:, :, 1], alpha=.01)\n",
        "ax1.set_title('Aligned landmarks cloud')\n",
        "\n",
        "# compute the mean shape \n",
        "plot_landmarks(mean_shape, ax=ax2)\n",
        "ax2.set_title('Mean landmarks');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JHIfSXrh61Dt"
      },
      "source": [
        "#### But let's do the same using Riemannian optimization!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OKTxZZYn61Du"
      },
      "source": [
        "**Q:** Why we need to optimize anything by hand, if we have the procrustes implemented in scipy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xetpYiSJ61Dv",
        "colab": {}
      },
      "source": [
        "import pymanopt as opt\n",
        "import pymanopt.manifolds as manifolds\n",
        "import pymanopt.solvers as solvers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SnUce3Al61Dx"
      },
      "source": [
        "Recall that the orthogonal Procrustus problem seeks for:\n",
        "$$\n",
        "R=\\arg \\min _{\\Omega }\\|\\Omega A-B\\|_{F}\\quad \\mathrm {subject\\ to} \\quad \\Omega ^{T}\\Omega =I,\n",
        "$$\n",
        "i.e. $R$ belongs to the Stiefel manifold. One can optimize that, however, it might be more reasonable to optimize using rotations + scaling.\n",
        "\n",
        "In here, $A$ and $B$ are our shapes, and $\\Omega$ is our seeked transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2POTE_YJ61Dz"
      },
      "source": [
        "**Exercise:** program the variants of the Procrustes alignment using the following variants:\n",
        " * $R \\in \\text{Stiefel}(2, 2)$, i.e. we seek a projection matrix using `Stiefel` object \n",
        " * $R \\in \\text{SO}(2)$, i.e. we seek a rotation matrix using `Rotations` object \n",
        " * $R \\in \\text{SO}(2)$ and $s \\in R^2$, i.e. we seek a rotation + scaling transform using `Product` of `Rotations` and `Euclidean` manifolds, see example [here](https://github.com/pymanopt/pymanopt/blob/master/examples/regression_offset_autograd.py))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IhCgjFUa61D0",
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np  # import here to avoid errors\n",
        "\n",
        "def riemannian_procrustes_projection(mean_shape, shape):\n",
        "    \"\"\"Align the source shape to the target shape using projection.\n",
        "\n",
        "    target_shape, source_shape: ndarrays of shape [35, 2]    \n",
        "    return ndarray of shape [35, 2]\n",
        "    \"\"\"\n",
        "    def cost(R):\n",
        "        return # <your code here>\n",
        "    solver = solvers.SteepestDescent()\n",
        "    manifold = # <your code here>manifolds.Stiefel(2, 2)\n",
        "    problem = opt.Problem(manifold=manifold, cost=cost, verbosity=0)\n",
        "    R_opt = solver.solve(problem)\n",
        "    return # <your code here>\n",
        "\n",
        "\n",
        "def riemannian_procrustes_rotation(mean_shape, shape):\n",
        "    \"\"\"Align the source shape to the target shape using rotation.\n",
        "\n",
        "    target_shape, source_shape: ndarrays of shape [35, 2]    \n",
        "    return ndarray of shape [35, 2]\n",
        "    \"\"\"\n",
        "    def cost(R):\n",
        "        return # <your code here>\n",
        "    solver = solvers.SteepestDescent()\n",
        "    manifold = # <your code here>\n",
        "    problem = opt.Problem(manifold=manifold, cost=cost, verbosity=0)\n",
        "    R_opt = solver.solve(problem)\n",
        "    return # <your code here>\n",
        "    \n",
        "    \n",
        "def riemannian_procrustes_rotation_scaling(mean_shape, shape):\n",
        "    \"\"\"Align the source shape to the target shape using a combination rotation and scaling.\n",
        "\n",
        "    target_shape, source_shape: ndarrays of shape [35, 2]    \n",
        "    return ndarray of shape [35, 2]\n",
        "    \"\"\"\n",
        "    def cost(Rs):\n",
        "        R, s = Rs\n",
        "        return # <your code here>\n",
        "    solver = solvers.SteepestDescent()\n",
        "    manifold = # <your code here>\n",
        "    problem = opt.Problem(manifold=manifold, cost=cost, verbosity=0)\n",
        "    Rs_opt = solver.solve(problem)\n",
        "    R_opt, s_opt = Rs_opt\n",
        "    return # <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0DmxJflX61D2",
        "colab": {}
      },
      "source": [
        "# Stiefel\n",
        "aligned_landmarks, mean_shape = align_landmarks(landmarks, aligner=riemannian_procrustes_projection, n_iterations=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GOKUOfcM61D3",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n",
        "ax1.scatter(aligned_landmarks[:, :, 0], -aligned_landmarks[:, :, 1], alpha=.01)\n",
        "ax1.set_title('Aligned landmarks cloud')\n",
        "\n",
        "# compute the mean shape \n",
        "plot_landmarks(mean_shape, ax=ax2)\n",
        "ax2.set_title('Mean landmarks');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gbXOO6gn61D4",
        "colab": {}
      },
      "source": [
        "# Rotations\n",
        "aligned_landmarks, mean_shape = align_landmarks(landmarks, aligner=riemannian_procrustes_rotation, n_iterations=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TItdt5Lc61D6",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n",
        "ax1.scatter(aligned_landmarks[:, :, 0], -aligned_landmarks[:, :, 1], alpha=.01)\n",
        "ax1.set_title('Aligned landmarks cloud')\n",
        "\n",
        "# compute the mean shape \n",
        "plot_landmarks(mean_shape, ax=ax2)\n",
        "ax2.set_title('Mean landmarks');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Idp0zCmT61D8",
        "colab": {}
      },
      "source": [
        "# Rotations + scale\n",
        "aligned_landmarks, mean_shape = align_landmarks(landmarks, aligner=riemannian_procrustes_rotation_scaling, n_iterations=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9yTFunjk61D9",
        "colab": {}
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n",
        "ax1.scatter(aligned_landmarks[:, :, 0], -aligned_landmarks[:, :, 1], alpha=.01)\n",
        "ax1.set_title('Aligned landmarks cloud')\n",
        "\n",
        "# compute the mean shape \n",
        "plot_landmarks(mean_shape, ax=ax2)\n",
        "ax2.set_title('Mean landmarks');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "857V-Nnb61D_"
      },
      "source": [
        "### PCA for learning the shape space\n",
        "_The goal of this section is to learn how to program the simple but powerful PCA linear dimensionality reduction technique using Riemannian optimization._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dtIx1kHQ61EA"
      },
      "source": [
        "The typical way of learning the shape space is to find a low-dimensional manifold controlling most of the variability in shapes in a (hopefully) interpretable way. Such a manifold is commonly found using [PCA method](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
        "\n",
        "We will apply PCA to a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times 70}$ of aligned shapes.\n",
        "\n",
        "A common way of learning PCA is using SVD implemented in the [`sklearn.decomposition.PCA` class](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8qpLLlGk61EA",
        "colab": {}
      },
      "source": [
        "aligned_landmarks = aligned_landmarks.reshape(-1, 70)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nHVAaflK61EB",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=1)\n",
        "pca.fit(aligned_landmarks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tQOIfJhk61ED",
        "colab": {}
      },
      "source": [
        "d0 = pca.inverse_transform(\n",
        "    pca.transform(aligned_landmarks)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GN914W3e61EE",
        "colab": {}
      },
      "source": [
        "data_scaled_vis = d0.reshape((-1, 35, 2))\n",
        "\n",
        "plt.scatter(data_scaled_vis[:200, :, 0], -data_scaled_vis[:200, :, 1], alpha=.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nz5W-GCw61EH"
      },
      "source": [
        "#### Do the same using Riemannian optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ahab-25161EH"
      },
      "source": [
        "Recall that PCA finds a low-dimensional linear subspace by searching for a corresponding orthogonal projection. Thus, PCA searches for an orthogonal projection $M$ such that:\n",
        "$$\n",
        "M = \\arg \\min _{\\Omega }\n",
        "    \\|X - \\Omega \\Omega^{\\intercal} X\\|^2_{F}\n",
        "    \\quad \n",
        "    \\mathrm {subject\\ to} \\quad \\Omega ^{T}\\Omega = I,\n",
        "$$\n",
        "i.e. $\\Omega$ belongs to the Stiefel manifold $\\mathcal{O}^{d \\times r}$. \n",
        "\n",
        "The value $\\|X - M M^{\\intercal} X\\|^2_{F}$ is the reconstruction error from projecting $X$ to $r$-dimensional subspace and restoring back to $d$-dimensional (original) one. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aq_xEV_Z61EI"
      },
      "source": [
        "**Exercise:** program the PCA by finding an orthogonal projection from 70-dimensional onto 2-dimensional subspace, using `pymanopt`.\n",
        "\n",
        "**Hint:** use `Stiefel(70, 2)` manifold and the reconstruction error cost as described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oxdv3xnz61EJ",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Cost function is the reconstruction error\n",
        "def cost(w):\n",
        "    return # <your code here>\n",
        "\n",
        "solver = solvers.TrustRegions()\n",
        "manifold = # <your code here>\n",
        "problem = opt.Problem(manifold=manifold, cost=cost)\n",
        "wopt = solver.solve(problem)\n",
        "\n",
        "print('The following projection matrix was found to minimise '\n",
        "      'the squared reconstruction error: ')\n",
        "print(wopt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2f5ZEYqp61EK"
      },
      "source": [
        "Now construct a low-dimensional approximation of $X$, by projecting to $r$-dimensional parameter space and back."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kcO2axlh61EK",
        "colab": {}
      },
      "source": [
        "aligned_landmarks_r = np.dot(wopt, np.dot(wopt.T, aligned_landmarks.T)).T\n",
        "aligned_landmarks_r = aligned_landmarks_r.reshape((-1, 35, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mXVcWGCx61EL",
        "colab": {}
      },
      "source": [
        "plt.scatter(aligned_landmarks_r[:200, :, 0], -aligned_landmarks_r[:200, :, 1], alpha=.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WZy0ws_s61EN"
      },
      "source": [
        "#### Exploring the lower-dimensional linear manifold parameterizing landmarks\n",
        "_The purpose of this part is to understand how the coordinate values in the lower-dimensional space influences the landmark shape_. \n",
        "\n",
        "Coordinates along principal components _parameterize_ the shape, i.e. smooth walk along these directions should result in interpolation between shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NxqNTtMj61EO"
      },
      "source": [
        "**Exercise:** explore the lower-dimensional linear manifold parameterizing landmarks:\n",
        " * Show samples _from the data_ with different coordinated along PC\\#1 (hint: use `reconstructions_sorted_along_pc` below)\n",
        " * Show _synthetic_ samples obtained by moving in the data manifold along PC\\#1 (hint: modify `reconstructions_sorted_along_pc` below into `vary_on_manifold`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "siVY17yR61EO",
        "colab": {}
      },
      "source": [
        "def reconstructions_sorted_along_pc(landmarks, w, pc=1, n_shapes=6):\n",
        "    # project to r-dimensional manifold\n",
        "    projected_landmarks = np.dot(w.T, landmarks.T).T\n",
        "    \n",
        "    # sort along dimension selected by pc\n",
        "    pc_idx = np.argsort(projected_landmarks[:, pc])\n",
        "    \n",
        "    # reconstruct several shapes with varying degree\n",
        "    # of expressiveness in parameter pc\n",
        "    idx = np.linspace(0, len(landmarks), n_shapes).astype(int)\n",
        "    idx[-1] = idx[-1] - 1\n",
        "    shapes_to_reconstruct = projected_landmarks[pc_idx[idx]].T\n",
        "    reconstructions = np.dot(w, shapes_to_reconstruct).T\n",
        "    reconstructions = reconstructions.reshape((-1, 35, 2))\n",
        "    \n",
        "    return reconstructions\n",
        "\n",
        "\n",
        "def plot_variability_along_pc(landmarks, w, pc=1, n_shapes=6):\n",
        "    reconstructions = reconstructions_sorted_along_pc(landmarks, w, pc=pc, n_shapes=n_shapes)\n",
        "    \n",
        "    fig, axs = plt.subplots(ncols=6, nrows=1, figsize=(18, 3))\n",
        "    for ax, image in zip(axs, reconstructions):\n",
        "        plot_landmarks(image, ax=ax)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kst15bLr61EP",
        "colab": {}
      },
      "source": [
        "plot_variability_along_pc? # <your code here> "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S-2qXv6h61EQ"
      },
      "source": [
        "**Q:** Would this variability necessary be exactly like the PCA?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gESFR4Ei61ER",
        "colab": {}
      },
      "source": [
        "# PC2\n",
        "def vary_on_manifold(landmarks, id, w, pc=1, n_shapes=6):\n",
        "    projected_landmarks = np.dot(w.T, landmarks.T).T\n",
        "    min_pc_value = # <your code here>\n",
        "    max_pc_value = # <your code here>\n",
        "    pc_values = # <your code here>\n",
        "    \n",
        "    the_one_projection = projected_landmarks[id][None]\n",
        "    shapes_to_reconstruct = np.tile(the_one_projection, (n_shapes, 1))\n",
        "    shapes_to_reconstruct[:, pc] = pc_values\n",
        "    \n",
        "    reconstructions = np.dot(w, shapes_to_reconstruct.T).T\n",
        "    reconstructions = reconstructions.reshape((-1, 35, 2))\n",
        "    \n",
        "    fig, axs = plt.subplots(ncols=n_shapes, nrows=1, figsize=(3 * n_shapes, 3))\n",
        "    for ax, image in zip(axs, reconstructions):\n",
        "        plot_landmarks(image, ax=ax)\n",
        "\n",
        "        \n",
        "vary_on_manifold(aligned_landmarks, 0, wopt, pc=1, n_shapes=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c20-j17b61ES"
      },
      "source": [
        "### Analysing the shape space of facial landmarks via MDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "493tgkMX61ET"
      },
      "source": [
        "#### Compute embedding of the shape space into 2D, preserving distances between shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JWwwL-JN61ET"
      },
      "source": [
        "Classic multidimensional scaling (MDS) aims to find an orthogonal mapping $M$ such that:\n",
        "$$\n",
        "M = \\arg \\min _{\\Omega } \n",
        "    \\sum_i \\sum_j (d_X (\\mathbf{x}_i, \\mathbf{x}_j) - \n",
        "        d_Y (\\Omega^{\\intercal}\\mathbf{x}_i, \\Omega^{\\intercal}\\mathbf{x}_j))^2\n",
        "    \\quad \n",
        "    \\mathrm {subject\\ to} \\quad \\Omega ^{T}\\Omega = I,\n",
        "$$\n",
        "i.e. $\\Omega$ belongs to the Stiefel manifold $\\mathcal{O}^{d \\times r}$ where $d$ is the dimensionality of the original space, and $r$ is the dimensionality of the compressed space.\n",
        "\n",
        "In other words, consider distances $d_X (\\mathbf{x}_i, \\mathbf{x}_j)$ between ech pair $(i, j)$ of objects in the original space $X$. MDS aims at projecting $\\mathbf{x}_i$'s to a linear subspace $Y$ such that each distance $d_Y (M^{\\intercal}\\mathbf{x}_i, M^{\\intercal}\\mathbf{x}_j)$ approximates $d_X (\\mathbf{x}_i, \\mathbf{x}_j)$ as closely as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TjIwEOLv61ET",
        "colab": {}
      },
      "source": [
        "aligned_landmarks = aligned_landmarks.reshape((-1, 70))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pB6dyZn861EV",
        "colab": {}
      },
      "source": [
        "# a slightly tricky way of computing pairwise distances for [n, d] matrixes of objects, \n",
        "# see https://stackoverflow.com/questions/28687321/computing-euclidean-distance-for-numpy-in-python\n",
        "\n",
        "def calculate_pairwise_distances(points):\n",
        "    return ((points[..., None] - points[..., None].T) ** 2).sum(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pZuE2xQC61EW",
        "colab": {}
      },
      "source": [
        "euclidean_distances = calculate_pairwise_distances(aligned_landmarks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U4ivakWM61EX"
      },
      "source": [
        "**Exercise:** program MDS dimensionality reduction method using `pymanopt`. Project from 70-dimensional to 2-dimensional space.\n",
        "\n",
        "**Hint:** to compute distances, use `calculate_pairwise_distances` above.\n",
        "\n",
        "**Hint:** use `Stiefel(70, 2)` manifold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bGBY_pT61EZ",
        "colab": {}
      },
      "source": [
        "import autograd.numpy as np\n",
        "\n",
        "def cost(w):\n",
        "    # <your code here>\n",
        "    \n",
        "\n",
        "solver = solvers.TrustRegions()\n",
        "manifold = # <your code here>\n",
        "problem = opt.Problem(manifold=manifold, cost=cost)\n",
        "wopt = solver.solve(problem)\n",
        "\n",
        "print('The following projection matrix was found to minimise '\n",
        "      'the squared reconstruction error: ')\n",
        "print(wopt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3H5ywQmF61Ea",
        "colab": {}
      },
      "source": [
        "projected_shapes = np.dot(wopt.T, aligned_landmarks.T).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koJiLKYknN4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from riemannianoptimization.tutorial_helpers import prepare_html_for_visualization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuvSiYIhnN4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(prepare_html_for_visualization(projected_shapes, aligned_landmarks, scatterplot_size=[700, 700],\n",
        "                                    annotation_size=[100, 100], floating_annotation=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qexyfuRN61Em"
      },
      "source": [
        "## Learning the Gaussian mixture models for word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y7K_UKuy61Em"
      },
      "source": [
        "This part of the tutorial is in a separate notebook, `riemannian_opt_gmm_embeddings.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvQnnSdZ61Em"
      },
      "source": [
        "## Bibliography"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W2apBwKW61Em"
      },
      "source": [
        "\n",
        "This tutorial is in part inspired by the work _Cunningham, J. P., & Ghahramani, Z. (2015). [Linear dimensionality reduction: Survey, insights, and generalizations.](http://www.jmlr.org/papers/volume16/cunningham15a/cunningham15a.pdf) The Journal of Machine Learning Research, 16(1), 2859-2900._ Reading this work in full will help you greatly broaden your understanding of linear dimensionality reduction techniques, systematize your knowledge of optimization setups involved therein, and get an overview of this area.\n",
        "\n",
        "_Townsend, J., Koep, N., & Weichwald, S. (2016). [Pymanopt: A python toolbox for optimization on manifolds using automatic differentiation](http://jmlr.org/papers/volume17/16-177/16-177.pdf). The Journal of Machine Learning Research, 17(1), 4755-4759._\n",
        "\n",
        "_Boumal, N., Mishra, B., Absil, P. A., & Sepulchre, R. (2014). [Manopt, a Matlab toolbox for optimization on manifolds](http://www.jmlr.org/papers/volume15/boumal14a/boumal14a.pdf). The Journal of Machine Learning Research, 15(1), 1455-1459._\n",
        "\n",
        "This tutorial uses data and annotations from the two works\n",
        " _Belhumeur, P. N., Jacobs, D. W., Kriegman, D. J., & Kumar, N. (2013). [Localizing parts of faces using a consensus of exemplars](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.227.8441&rep=rep1&type=pdf). IEEE transactions on pattern analysis and machine intelligence, 35(12), 2930-2940._\n",
        " and \n",
        "_Huang, G. B., Mattar, M., Berg, T., & Learned-Miller, E. (2008, October)._ [Labeled faces in the wild: A database forstudying face recognition in unconstrained environments](https://hal.inria.fr/docs/00/32/19/23/PDF/Huang_long_eccv2008-lfw.pdf).\n"
      ]
    }
  ]
}